{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Squest Squest is a Web portal that allow to expose Ansible Tower based automation as a service. Features: Catalog of service that point to an Ansible Tower/AWX job template Admin approval on requests Instance lifecycle management (update, delete) If you want an idea of what you can do with Squest, click on the image below","title":"Home"},{"location":"#squest","text":"Squest is a Web portal that allow to expose Ansible Tower based automation as a service. Features: Catalog of service that point to an Ansible Tower/AWX job template Admin approval on requests Instance lifecycle management (update, delete) If you want an idea of what you can do with Squest, click on the image below","title":"Squest"},{"location":"deployment/","text":"Deploy Squest The current deployment is based on Docker compose for testing the tool. Pre-requisites: docker docker-compose To test the application, run the full env docker compose file docker-compose -f dev-env.docker-compose.yml -f full-env.docker-compose.yml up Then connect with your web browser to http://127.0.0.1:8000 The default admin account is admin // admin","title":"Deployment"},{"location":"deployment/#deploy-squest","text":"The current deployment is based on Docker compose for testing the tool. Pre-requisites: docker docker-compose To test the application, run the full env docker compose file docker-compose -f dev-env.docker-compose.yml -f full-env.docker-compose.yml up Then connect with your web browser to http://127.0.0.1:8000 The default admin account is admin // admin","title":"Deploy Squest"},{"location":"lifecycle_management/","text":"Lifecycle management Provisioning a service When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : instance : id : 1 name : test service : 1 spec : {} state : PROVISIONING file_name : foo.conf Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project) Playbook example In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job tempalate survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}admin/instance/{{ squest['instance']['id'] }}/\" # do not forget the last slash user : \"admin\" password : \"admin\" method : PATCH body : spec : file_name : \"{{ file_name }}\" force_basic_auth : yes status_code : 200 body_format : json Day 2 operations Day 2 operations are operations that update or delete existing resources. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated Playbook example Example of extra vars sent by squest: squest : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job tempalate survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes","title":"Lifecycle management"},{"location":"lifecycle_management/#lifecycle-management","text":"","title":"Lifecycle management"},{"location":"lifecycle_management/#provisioning-a-service","text":"When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : instance : id : 1 name : test service : 1 spec : {} state : PROVISIONING file_name : foo.conf Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project)","title":"Provisioning a service"},{"location":"lifecycle_management/#playbook-example","text":"In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job tempalate survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}admin/instance/{{ squest['instance']['id'] }}/\" # do not forget the last slash user : \"admin\" password : \"admin\" method : PATCH body : spec : file_name : \"{{ file_name }}\" force_basic_auth : yes status_code : 200 body_format : json","title":"Playbook example"},{"location":"lifecycle_management/#day-2-operations","text":"Day 2 operations are operations that update or delete existing resources. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated","title":"Day 2 operations"},{"location":"lifecycle_management/#playbook-example_1","text":"Example of extra vars sent by squest: squest : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job tempalate survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes","title":"Playbook example"},{"location":"squest_settings/","text":"Configure settings Settings are placed into the squest/settings/development.py file which is a standard Django core settings file LDAP backend LDAP can be activated by setting the environment vairable LDAP_ENABLED to True or directly in the settings.py file: LDAP_ENABLED = True The configuration file need then to be created in Squest/ldap_config.py . The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/path/to/my/company-ca.crt\" AUTH_LDAP_CONNECTION_OPTIONS : { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_START_TLS : True AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" } Email Email settings are based on the django settings EMAIL_HOST = os . environ . get ( 'EMAIL_HOST' , None ) EMAIL_PORT = 25","title":"Squest"},{"location":"squest_settings/#configure-settings","text":"Settings are placed into the squest/settings/development.py file which is a standard Django core settings file","title":"Configure settings"},{"location":"squest_settings/#ldap-backend","text":"LDAP can be activated by setting the environment vairable LDAP_ENABLED to True or directly in the settings.py file: LDAP_ENABLED = True The configuration file need then to be created in Squest/ldap_config.py . The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/path/to/my/company-ca.crt\" AUTH_LDAP_CONNECTION_OPTIONS : { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_START_TLS : True AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" }","title":"LDAP backend"},{"location":"squest_settings/#email","text":"Email settings are based on the django settings EMAIL_HOST = os . environ . get ( 'EMAIL_HOST' , None ) EMAIL_PORT = 25","title":"Email"},{"location":"tower_settings/","text":"Tower configuration Squest will need a token in order to communicate with your Tower instance. Create an application on your Tower/AWX instance On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential Create a token for Squest application On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Tower"},{"location":"tower_settings/#tower-configuration","text":"Squest will need a token in order to communicate with your Tower instance.","title":"Tower configuration"},{"location":"tower_settings/#create-an-application-on-your-towerawx-instance","text":"On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential","title":"Create an application on your Tower/AWX instance"},{"location":"tower_settings/#create-a-token-for-squest-application","text":"On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Create a token for Squest application"},{"location":"contribute/documentation/","text":"Contributing to the documentation The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve The page is available on http://127.0.0.1:8000 . Send a pull request then to propose your changes to the project.","title":"Documentation"},{"location":"contribute/documentation/#contributing-to-the-documentation","text":"The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve The page is available on http://127.0.0.1:8000 . Send a pull request then to propose your changes to the project.","title":"Contributing to the documentation"},{"location":"dev/db-erd/","text":"Database Entity Relationship Diagrams erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type json enabled_survey_fields bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date date_message string content } SUPPORT_MESSAGE { date date_message string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database ERD"},{"location":"dev/db-erd/#database-entity-relationship-diagrams","text":"erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type json enabled_survey_fields bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date date_message string content } SUPPORT_MESSAGE { date date_message string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database Entity Relationship Diagrams"},{"location":"dev/dev-env/","text":"Setup a development environment Pre requisites Tools Following tools need to be installed on your workstation: Docker Docker-compose Python 3.8 Python virtualenv Poetry npm System packages Ubuntu based OS: sudo apt-get install libmysqlclient-dev CentOS/RedHat/Fedora sudo yum install mysql-devel Start a development environment The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API Docker compose Run the Docker dev env to bring up database, message broker and other required system docker-compose -f dev-env.docker-compose.yml up Javascript libraries Install JS libs (npm need to be installed) npm install Python environment Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data Celery worker and periodic task scheduler Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat --scheduler django -l info Django integrated web server This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver Commands To clean all Celery pending tasks poetry shell celery -A restapi purge Execute tests Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test coverage report phpMyAdmin phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"Setup a dev env"},{"location":"dev/dev-env/#setup-a-development-environment","text":"","title":"Setup a development environment"},{"location":"dev/dev-env/#pre-requisites","text":"","title":"Pre requisites"},{"location":"dev/dev-env/#tools","text":"Following tools need to be installed on your workstation: Docker Docker-compose Python 3.8 Python virtualenv Poetry npm","title":"Tools"},{"location":"dev/dev-env/#system-packages","text":"Ubuntu based OS: sudo apt-get install libmysqlclient-dev CentOS/RedHat/Fedora sudo yum install mysql-devel","title":"System packages"},{"location":"dev/dev-env/#start-a-development-environment","text":"The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API","title":"Start a development environment"},{"location":"dev/dev-env/#docker-compose","text":"Run the Docker dev env to bring up database, message broker and other required system docker-compose -f dev-env.docker-compose.yml up","title":"Docker compose"},{"location":"dev/dev-env/#javascript-libraries","text":"Install JS libs (npm need to be installed) npm install","title":"Javascript libraries"},{"location":"dev/dev-env/#python-environment","text":"Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data","title":"Python environment"},{"location":"dev/dev-env/#celery-worker-and-periodic-task-scheduler","text":"Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat --scheduler django -l info","title":"Celery worker and periodic task scheduler"},{"location":"dev/dev-env/#django-integrated-web-server","text":"This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver","title":"Django integrated web server"},{"location":"dev/dev-env/#commands","text":"To clean all Celery pending tasks poetry shell celery -A restapi purge","title":"Commands"},{"location":"dev/dev-env/#execute-tests","text":"Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test coverage report","title":"Execute tests"},{"location":"dev/dev-env/#phpmyadmin","text":"phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"phpMyAdmin"},{"location":"dev/instance-state-machine/","text":"Instance state machine graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/instance-state-machine/#instance-state-machine","text":"graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/request-state-machine/","text":"Request state machine graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted","title":"Request state machine"},{"location":"dev/request-state-machine/#request-state-machine","text":"graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted","title":"Request state machine"}]}