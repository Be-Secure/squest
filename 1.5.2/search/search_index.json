{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Squest The current deployment is based on Docker compose. Pre-requisites: docker docker-compose To run the application, execute the docker compose file docker-compose up Then connect with your web browser to http://127.0.0.1:8080 The default admin account is admin // admin The default export the port 8080. If you want to use the standard HTTP port 80, update the file docker-compose.override.yml . services : nginx : ports : - \"80:8080\" More details are available in the \"Installation\" section of this documentation.","title":"Home"},{"location":"#squest","text":"The current deployment is based on Docker compose. Pre-requisites: docker docker-compose To run the application, execute the docker compose file docker-compose up Then connect with your web browser to http://127.0.0.1:8080 The default admin account is admin // admin The default export the port 8080. If you want to use the standard HTTP port 80, update the file docker-compose.override.yml . services : nginx : ports : - \"80:8080\" More details are available in the \"Installation\" section of this documentation.","title":"Squest"},{"location":"configuration/squest_settings/","text":"Configuration settings Default settings are configured to provide a testing/development environment. For a production setup it is recommended to adjust them following you production target environment. The configuration is loaded from environment variables file placed in the folder docker/environment_variables . Database MYSQL_DATABASE Default: squest_db Mysql database name. MYSQL_USER Default: squest_user Mysql user used to connect to the MYSQL_DATABASE name. MYSQL_PASSWORD Default: squest_password Password of the MYSQL_USER username. MYSQL_HOST Default: 127.0.0.1 Mysql database host. The default value is localhost to match the development configuration. Switch to db in production when using the docker-compose based deployment. MYSQL_PORT Default: 3306 Mysql database port. Authentication LDAP_ENABLED Default: False Set to True to enable LDAP based authentication. See configuration doc . Squest SQUEST_HOST Default: http://squest.domain.local Address of the Squest portal instance. Used in email templates and in metadata sent to Tower job templates. SQUEST_EMAIL_HOST Default: squest.domain.local Domain name used as email sender. E.g: \"squest@squest.domain.local\". SQUEST_EMAIL_NOTIFICATION_ENABLED Default: Based on DEBUG value by default Set to True to enable email notifications. SMTP EMAIL_HOST Default: localhost The SMTP host to use for sending email. EMAIL_PORT Default: 25 Port to use for the SMTP server defined in EMAIL_HOST . Backup BACKUP_ENABLED Default: False Switch to True to enable backup. Refer to the dedicated documentation . BACKUP_CRONTAB Default: 0 1 * * * Crontab line for backup. By default, the backup is performed every day at 1AM. DBBACKUP_CLEANUP_KEEP Default: 5 Number of db backup file to keep. Doc . DBBACKUP_CLEANUP_KEEP_MEDIA Default: 5 Number of media backup tar to keep. Doc . Metrics METRICS_ENABLED Default: False Switch to True to enable Prometheus metrics page. METRICS_PASSWORD_PROTECTED Default: True Switch to False to disable the basic authentication on metrics page. METRICS_AUTHORIZATION_USERNAME Default: admin Username for the basic authentication of the metrics page. METRICS_AUTHORIZATION_PASSWORD Default: admin Password for the basic authentication of the metrics page. Auto cleanup DOC_IMAGES_CLEANUP_ENABLED Default: False Switch to True to enable automatic cleanup of ghost docs images from media folder. DOC_IMAGES_CLEANUP_CRONTAB Default: 30 1 * * * Crontab line for ghost image cleanup. By default performed every day at 1:30 AM. Production SECRET_KEY Default: Default randomly-generated Django secret key used for cryptographic signing. Doc . DEBUG Default: True Django DEBUG mode. Switch to False for production. ALLOWED_HOSTS Default: * Comma separated list of allowed FQDN. Refer to the complete documentation . CELERY_BROKER_URL Default: amqp://rabbitmq:rabbitmq@localhost:5672/squest RabbitMQ message broker URL. The default value is localhost to match the development configuration. Replace localhost by rabbitmq in production when using the docker-compose based deployment. CELERY_TASK_SOFT_TIME_LIMIT Default: 300 Async task execution timeout. Doc . LANGUAGE_CODE Default: en-us Django language. Doc TIME_ZONE Default: Europe/Paris Time zone of the server that host Squest service. Doc","title":"Squest"},{"location":"configuration/squest_settings/#configuration-settings","text":"Default settings are configured to provide a testing/development environment. For a production setup it is recommended to adjust them following you production target environment. The configuration is loaded from environment variables file placed in the folder docker/environment_variables .","title":"Configuration  settings"},{"location":"configuration/squest_settings/#database","text":"","title":"Database"},{"location":"configuration/squest_settings/#mysql_database","text":"Default: squest_db Mysql database name.","title":"MYSQL_DATABASE"},{"location":"configuration/squest_settings/#mysql_user","text":"Default: squest_user Mysql user used to connect to the MYSQL_DATABASE name.","title":"MYSQL_USER"},{"location":"configuration/squest_settings/#mysql_password","text":"Default: squest_password Password of the MYSQL_USER username.","title":"MYSQL_PASSWORD"},{"location":"configuration/squest_settings/#mysql_host","text":"Default: 127.0.0.1 Mysql database host. The default value is localhost to match the development configuration. Switch to db in production when using the docker-compose based deployment.","title":"MYSQL_HOST"},{"location":"configuration/squest_settings/#mysql_port","text":"Default: 3306 Mysql database port.","title":"MYSQL_PORT"},{"location":"configuration/squest_settings/#authentication","text":"","title":"Authentication"},{"location":"configuration/squest_settings/#ldap_enabled","text":"Default: False Set to True to enable LDAP based authentication. See configuration doc .","title":"LDAP_ENABLED"},{"location":"configuration/squest_settings/#squest","text":"","title":"Squest"},{"location":"configuration/squest_settings/#squest_host","text":"Default: http://squest.domain.local Address of the Squest portal instance. Used in email templates and in metadata sent to Tower job templates.","title":"SQUEST_HOST"},{"location":"configuration/squest_settings/#squest_email_host","text":"Default: squest.domain.local Domain name used as email sender. E.g: \"squest@squest.domain.local\".","title":"SQUEST_EMAIL_HOST"},{"location":"configuration/squest_settings/#squest_email_notification_enabled","text":"Default: Based on DEBUG value by default Set to True to enable email notifications.","title":"SQUEST_EMAIL_NOTIFICATION_ENABLED"},{"location":"configuration/squest_settings/#smtp","text":"","title":"SMTP"},{"location":"configuration/squest_settings/#email_host","text":"Default: localhost The SMTP host to use for sending email.","title":"EMAIL_HOST"},{"location":"configuration/squest_settings/#email_port","text":"Default: 25 Port to use for the SMTP server defined in EMAIL_HOST .","title":"EMAIL_PORT"},{"location":"configuration/squest_settings/#backup","text":"","title":"Backup"},{"location":"configuration/squest_settings/#backup_enabled","text":"Default: False Switch to True to enable backup. Refer to the dedicated documentation .","title":"BACKUP_ENABLED"},{"location":"configuration/squest_settings/#backup_crontab","text":"Default: 0 1 * * * Crontab line for backup. By default, the backup is performed every day at 1AM.","title":"BACKUP_CRONTAB"},{"location":"configuration/squest_settings/#dbbackup_cleanup_keep","text":"Default: 5 Number of db backup file to keep. Doc .","title":"DBBACKUP_CLEANUP_KEEP"},{"location":"configuration/squest_settings/#dbbackup_cleanup_keep_media","text":"Default: 5 Number of media backup tar to keep. Doc .","title":"DBBACKUP_CLEANUP_KEEP_MEDIA"},{"location":"configuration/squest_settings/#metrics","text":"","title":"Metrics"},{"location":"configuration/squest_settings/#metrics_enabled","text":"Default: False Switch to True to enable Prometheus metrics page.","title":"METRICS_ENABLED"},{"location":"configuration/squest_settings/#metrics_password_protected","text":"Default: True Switch to False to disable the basic authentication on metrics page.","title":"METRICS_PASSWORD_PROTECTED"},{"location":"configuration/squest_settings/#metrics_authorization_username","text":"Default: admin Username for the basic authentication of the metrics page.","title":"METRICS_AUTHORIZATION_USERNAME"},{"location":"configuration/squest_settings/#metrics_authorization_password","text":"Default: admin Password for the basic authentication of the metrics page.","title":"METRICS_AUTHORIZATION_PASSWORD"},{"location":"configuration/squest_settings/#auto-cleanup","text":"","title":"Auto cleanup"},{"location":"configuration/squest_settings/#doc_images_cleanup_enabled","text":"Default: False Switch to True to enable automatic cleanup of ghost docs images from media folder.","title":"DOC_IMAGES_CLEANUP_ENABLED"},{"location":"configuration/squest_settings/#doc_images_cleanup_crontab","text":"Default: 30 1 * * * Crontab line for ghost image cleanup. By default performed every day at 1:30 AM.","title":"DOC_IMAGES_CLEANUP_CRONTAB"},{"location":"configuration/squest_settings/#production","text":"","title":"Production"},{"location":"configuration/squest_settings/#secret_key","text":"Default: Default randomly-generated Django secret key used for cryptographic signing. Doc .","title":"SECRET_KEY"},{"location":"configuration/squest_settings/#debug","text":"Default: True Django DEBUG mode. Switch to False for production.","title":"DEBUG"},{"location":"configuration/squest_settings/#allowed_hosts","text":"Default: * Comma separated list of allowed FQDN. Refer to the complete documentation .","title":"ALLOWED_HOSTS"},{"location":"configuration/squest_settings/#celery_broker_url","text":"Default: amqp://rabbitmq:rabbitmq@localhost:5672/squest RabbitMQ message broker URL. The default value is localhost to match the development configuration. Replace localhost by rabbitmq in production when using the docker-compose based deployment.","title":"CELERY_BROKER_URL"},{"location":"configuration/squest_settings/#celery_task_soft_time_limit","text":"Default: 300 Async task execution timeout. Doc .","title":"CELERY_TASK_SOFT_TIME_LIMIT"},{"location":"configuration/squest_settings/#language_code","text":"Default: en-us Django language. Doc","title":"LANGUAGE_CODE"},{"location":"configuration/squest_settings/#time_zone","text":"Default: Europe/Paris Time zone of the server that host Squest service. Doc","title":"TIME_ZONE"},{"location":"configuration/tower_settings/","text":"Tower configuration Squest will need a token in order to communicate with your Tower instance. Create an application on your Tower/AWX instance On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential Create a token for Squest application On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Tower"},{"location":"configuration/tower_settings/#tower-configuration","text":"Squest will need a token in order to communicate with your Tower instance.","title":"Tower configuration"},{"location":"configuration/tower_settings/#create-an-application-on-your-towerawx-instance","text":"On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential","title":"Create an application on your Tower/AWX instance"},{"location":"configuration/tower_settings/#create-a-token-for-squest-application","text":"On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Create a token for Squest application"},{"location":"contribute/code/","text":"Contributing: code The community can contribute to Squest by providing some new features, bug fix and enhancements. How to contribute Fork it! Checkout the dev branch git checkout dev Create your feature branch: git checkout -b my-new-feature Commit your changes: git commit -am 'Add some feature' Push to the branch: git push origin my-new-feature Submit a pull request in the dev branch If you are new on Github environment, we recommend you to read the first contribution guide . Follow the development environment setup documentation to prepare your workstation with prerequisites. Constraints Respect PEP 257 -- Docstring conventions. For each class or method add a description with summary, input parameter, returned parameter, type of parameter def my_method ( my_parameter ): \"\"\" Description of he method :param my_parameter: description of he parameter :type my_parameter: str \"\"\" pass Respect PEP 8 -- Style Guide for Python Code We recommend the usage of an IDE like Pycharm","title":"Code"},{"location":"contribute/code/#contributing-code","text":"The community can contribute to Squest by providing some new features, bug fix and enhancements. How to contribute Fork it! Checkout the dev branch git checkout dev Create your feature branch: git checkout -b my-new-feature Commit your changes: git commit -am 'Add some feature' Push to the branch: git push origin my-new-feature Submit a pull request in the dev branch If you are new on Github environment, we recommend you to read the first contribution guide . Follow the development environment setup documentation to prepare your workstation with prerequisites.","title":"Contributing: code"},{"location":"contribute/code/#constraints","text":"Respect PEP 257 -- Docstring conventions. For each class or method add a description with summary, input parameter, returned parameter, type of parameter def my_method ( my_parameter ): \"\"\" Description of he method :param my_parameter: description of he parameter :type my_parameter: str \"\"\" pass Respect PEP 8 -- Style Guide for Python Code We recommend the usage of an IDE like Pycharm","title":"Constraints"},{"location":"contribute/documentation/","text":"Contributing to the documentation The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve -a 0 .0.0.0:4000 The page is available on http://127.0.0.1:4000 . Send a pull request then to propose your changes to the project. Notes Reset your gh-pages branch to match the upstream If you've built mkdocs and published a version in your fork for testing, your gh-pages branch will differ from the upstream repository. To reset your local gh-pages , follow the procedure below: # delete local branch git branch -D gh-pages # delete remote branch (fork here is your remote. Replace by origin if needed) git push -d fork gh-pages # checkout gh-pages git checkout --orphan gh-pages # pull last version (upstream is the remote name of the main repo) git pull upstream gh-pages # (optional) force push to your fork to override changes git push -f fork gh-pages # go back to your original branch git checkout master","title":"Documentation"},{"location":"contribute/documentation/#contributing-to-the-documentation","text":"The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve -a 0 .0.0.0:4000 The page is available on http://127.0.0.1:4000 . Send a pull request then to propose your changes to the project.","title":"Contributing to the documentation"},{"location":"contribute/documentation/#notes","text":"","title":"Notes"},{"location":"contribute/documentation/#reset-your-gh-pages-branch-to-match-the-upstream","text":"If you've built mkdocs and published a version in your fork for testing, your gh-pages branch will differ from the upstream repository. To reset your local gh-pages , follow the procedure below: # delete local branch git branch -D gh-pages # delete remote branch (fork here is your remote. Replace by origin if needed) git push -d fork gh-pages # checkout gh-pages git checkout --orphan gh-pages # pull last version (upstream is the remote name of the main repo) git pull upstream gh-pages # (optional) force push to your fork to override changes git push -f fork gh-pages # go back to your original branch git checkout master","title":"Reset your gh-pages branch to match the upstream"},{"location":"dev/db-erd/","text":"Database Entity Relationship Diagrams erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date creation_date string content } SUPPORT_MESSAGE { date creation_date string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database ERD"},{"location":"dev/db-erd/#database-entity-relationship-diagrams","text":"erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date creation_date string content } SUPPORT_MESSAGE { date creation_date string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database Entity Relationship Diagrams"},{"location":"dev/dev-env/","text":"Setup a development environment Pre requisites Tools Following tools need to be installed on your workstation: Docker Docker-compose Python 3.9 Python virtualenv Poetry npm 8 System packages Ubuntu based OS: sudo apt-get install libmysqlclient-dev graphviz default-mysql-client libsqlite3-dev CentOS/RedHat/Fedora sudo yum install mysql-devel graphviz mysql libsq3-devel Start a development environment The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API Docker compose Run the Docker compose file with only required services to bring up database, message broker and other required system docker-compose up db phpmyadmin rabbitmq Javascript libraries Install JS libs (npm need to be installed) npm install Python environment Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data Celery worker and periodic task scheduler Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat -l info Django integrated web server This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver Settings are placed into the squest/settings.py file which is a standard Django core settings file . Commands To clean all Celery pending tasks poetry shell celery -A restapi purge Execute tests Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test # generate terminal report coverage report # generate HTML report coverage html phpMyAdmin phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"Setup a dev env"},{"location":"dev/dev-env/#setup-a-development-environment","text":"","title":"Setup a development environment"},{"location":"dev/dev-env/#pre-requisites","text":"","title":"Pre requisites"},{"location":"dev/dev-env/#tools","text":"Following tools need to be installed on your workstation: Docker Docker-compose Python 3.9 Python virtualenv Poetry npm 8","title":"Tools"},{"location":"dev/dev-env/#system-packages","text":"Ubuntu based OS: sudo apt-get install libmysqlclient-dev graphviz default-mysql-client libsqlite3-dev CentOS/RedHat/Fedora sudo yum install mysql-devel graphviz mysql libsq3-devel","title":"System packages"},{"location":"dev/dev-env/#start-a-development-environment","text":"The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API","title":"Start a development environment"},{"location":"dev/dev-env/#docker-compose","text":"Run the Docker compose file with only required services to bring up database, message broker and other required system docker-compose up db phpmyadmin rabbitmq","title":"Docker compose"},{"location":"dev/dev-env/#javascript-libraries","text":"Install JS libs (npm need to be installed) npm install","title":"Javascript libraries"},{"location":"dev/dev-env/#python-environment","text":"Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data","title":"Python environment"},{"location":"dev/dev-env/#celery-worker-and-periodic-task-scheduler","text":"Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat -l info","title":"Celery worker and periodic task scheduler"},{"location":"dev/dev-env/#django-integrated-web-server","text":"This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver Settings are placed into the squest/settings.py file which is a standard Django core settings file .","title":"Django integrated web server"},{"location":"dev/dev-env/#commands","text":"To clean all Celery pending tasks poetry shell celery -A restapi purge","title":"Commands"},{"location":"dev/dev-env/#execute-tests","text":"Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test # generate terminal report coverage report # generate HTML report coverage html","title":"Execute tests"},{"location":"dev/dev-env/#phpmyadmin","text":"phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"phpMyAdmin"},{"location":"dev/instance-state-machine/","text":"Instance state machine graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/instance-state-machine/#instance-state-machine","text":"graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/request-state-machine/","text":"Request state machine graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled submitted -->|reject| rejected canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess accepted -->|reject| rejected auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted archived[ARCHIVED] complete -->|archive| archived archived -->|unarchive| complete","title":"Request state machine"},{"location":"dev/request-state-machine/#request-state-machine","text":"graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled submitted -->|reject| rejected canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess accepted -->|reject| rejected auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted archived[ARCHIVED] complete -->|archive| archived archived -->|unarchive| complete","title":"Request state machine"},{"location":"installation/backup/","text":"Backup Persistent data of squest are: database media folder (used to store images) An integrated backup solution based on django-dbbackup is available. Once enabled, backups are placed in the /app/backup folder of the celery-beat container. Execute a backup manually Execute the command below against the celery-beat container: docker-compose exec celery-beat python manage.py dbbackup --clean docker-compose exec celery-beat python manage.py mediabackup --clean Get the backup list docker-compose exec celery-beat python manage.py listbackups Output example: Name Datetime default-3095326a6ee7-2021-09-10-112953.dump 09/10/21 11:29:53 3095326a6ee7-2021-09-10-113338.tar 09/10/21 11:33:38 Data are placed by default in a mounted volume named squest_backup . You can get the real path on the host by inspecting the volume: docker volume inspect squest_backup Output example: [ { \"CreatedAt\" : \"2021-09-13T09:42:26+02:00\" , \"Driver\" : \"local\" , \"Labels\" : { \"com.docker.compose.project\" : \"squest\" , \"com.docker.compose.version\" : \"1.28.4\" , \"com.docker.compose.volume\" : \"backup\" }, \"Mountpoint\" : \"/var/lib/docker/volumes/squest_backup/_data\" , \"Name\" : \"squest_backup\" , \"Options\" : null , \"Scope\" : \"local\" } ] In this example, data are placed in the mount point /var/lib/docker/volumes/squest_backup/_data on the host. Files in this path need to be placed in a safe place. Enable automatic backup Enable automatic backup by updating your environment configuration file docker/environment_variables/squest.env : BACKUP_ENABLED = True By default, backup is performed every day at 1 AM. Note Follow the full configuration documentation to know all available flags for the backup service. Restore Start Squest services like for the initial deployment docker-compose up Copy you backup files into the squest_backup mount point of your host sudo cp <backup_folder_path>/* <squest_backup_mount_point> E.g: sudo cp ~/Desktop/squest_backup/* /var/lib/docker/volumes/squest_backup/_data/ Check that the tool can list your backup files docker-compose exec celery-beat python manage.py listbackups Restore the database and media folder docker-compose exec celery-beat python manage.py dbrestore docker-compose exec celery-beat python manage.py mediarestore Note Get more info on dbrestore and mediarestore command arguments on the official doc .","title":"Backup"},{"location":"installation/backup/#backup","text":"Persistent data of squest are: database media folder (used to store images) An integrated backup solution based on django-dbbackup is available. Once enabled, backups are placed in the /app/backup folder of the celery-beat container.","title":"Backup"},{"location":"installation/backup/#execute-a-backup-manually","text":"Execute the command below against the celery-beat container: docker-compose exec celery-beat python manage.py dbbackup --clean docker-compose exec celery-beat python manage.py mediabackup --clean Get the backup list docker-compose exec celery-beat python manage.py listbackups Output example: Name Datetime default-3095326a6ee7-2021-09-10-112953.dump 09/10/21 11:29:53 3095326a6ee7-2021-09-10-113338.tar 09/10/21 11:33:38 Data are placed by default in a mounted volume named squest_backup . You can get the real path on the host by inspecting the volume: docker volume inspect squest_backup Output example: [ { \"CreatedAt\" : \"2021-09-13T09:42:26+02:00\" , \"Driver\" : \"local\" , \"Labels\" : { \"com.docker.compose.project\" : \"squest\" , \"com.docker.compose.version\" : \"1.28.4\" , \"com.docker.compose.volume\" : \"backup\" }, \"Mountpoint\" : \"/var/lib/docker/volumes/squest_backup/_data\" , \"Name\" : \"squest_backup\" , \"Options\" : null , \"Scope\" : \"local\" } ] In this example, data are placed in the mount point /var/lib/docker/volumes/squest_backup/_data on the host. Files in this path need to be placed in a safe place.","title":"Execute a backup manually"},{"location":"installation/backup/#enable-automatic-backup","text":"Enable automatic backup by updating your environment configuration file docker/environment_variables/squest.env : BACKUP_ENABLED = True By default, backup is performed every day at 1 AM. Note Follow the full configuration documentation to know all available flags for the backup service.","title":"Enable automatic backup"},{"location":"installation/backup/#restore","text":"Start Squest services like for the initial deployment docker-compose up Copy you backup files into the squest_backup mount point of your host sudo cp <backup_folder_path>/* <squest_backup_mount_point> E.g: sudo cp ~/Desktop/squest_backup/* /var/lib/docker/volumes/squest_backup/_data/ Check that the tool can list your backup files docker-compose exec celery-beat python manage.py listbackups Restore the database and media folder docker-compose exec celery-beat python manage.py dbrestore docker-compose exec celery-beat python manage.py mediarestore Note Get more info on dbrestore and mediarestore command arguments on the official doc .","title":"Restore"},{"location":"installation/ldap/","text":"LDAP authentication backend LDAP can be activated by setting the environment variable LDAP_ENABLED to True in your configuration: LDAP_ENABLED = True The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com:636\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/usr/local/share/ca-certificates/ldap_ca.crt\" # default path in ldap docker compose file AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" } Update the ldap.docker-compose.yml file to mount your configuration file and the CA certificate of the LDAP server (if LDAPS is used) in django and celery containers: django : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt celery-worker : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt celery-beat : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt Run docker compose with the ldap config docker-compose -f docker-compose.yml -f docker-compose.override.yml -f ldap.docker-compose.yml up","title":"LDAP"},{"location":"installation/ldap/#ldap-authentication-backend","text":"LDAP can be activated by setting the environment variable LDAP_ENABLED to True in your configuration: LDAP_ENABLED = True The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com:636\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/usr/local/share/ca-certificates/ldap_ca.crt\" # default path in ldap docker compose file AUTH_LDAP_CONNECTION_OPTIONS = { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" } Update the ldap.docker-compose.yml file to mount your configuration file and the CA certificate of the LDAP server (if LDAPS is used) in django and celery containers: django : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt celery-worker : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt celery-beat : volumes : - ./Squest/ldap_config.py:/app/Squest/ldap_config.py - ./docker/certs/ldap_ca.crt:/usr/local/share/ca-certificates/ldap_ca.crt Run docker compose with the ldap config docker-compose -f docker-compose.yml -f docker-compose.override.yml -f ldap.docker-compose.yml up","title":"LDAP authentication backend"},{"location":"installation/tls/","text":"TLS This section explains how to add TLS support on Squest. The TLS endpoint is managed by a reverse proxy on top of the default web server. This is not the only way to handle this part. Many tools like Nginx, Apache or Traefik could be used, and you are free to use the one you want instead of this proposed configuration. The only recommendation we have is to keep the default nginx web server as main http entrypoint. TLS using Caddy Caddy is a powerful webserver written in Go which provide a reverse proxy feature. In the example below, we'll use self-signed certificate. Follow the official documentation if you want to configure it to use an ACME like \"Let's Encrypt\" instead. Place your certificate and key file in the folder docker/certs . E.g: docker \u251c\u2500\u2500 Caddyfile \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 squest.crt \u2502 \u2514\u2500\u2500 squest.key Update the docker/Caddyfile with the FQDN of your server. By default, the FQDN is set to squest.domain.local squest.domain.local { # This line should match the ALLOWED_HOSTS in your Squest environment reverse_proxy nginx:8080 encode gzip zstd tls /etc/ssl/private/squest.crt /etc/ssl/private/squest.key # or: # tls /etc/ssl/private/cert.pem log { level error } } Update the ALLOWED_HOSTS environment variable from the configuration file docker/environment_variables/squest.env to match your FQDN. ALLOWED_HOSTS = squest.domain.local Start docker compose with the TLS configuration: docker-compose -f docker-compose.yml -f tls.docker-compose.yml up The squest service is then reachable via HTTP and HTTPS standard ports (80/443). http://squest.domain.local https://squest.domain.local","title":"TLS"},{"location":"installation/tls/#tls","text":"This section explains how to add TLS support on Squest. The TLS endpoint is managed by a reverse proxy on top of the default web server. This is not the only way to handle this part. Many tools like Nginx, Apache or Traefik could be used, and you are free to use the one you want instead of this proposed configuration. The only recommendation we have is to keep the default nginx web server as main http entrypoint.","title":"TLS"},{"location":"installation/tls/#tls-using-caddy","text":"Caddy is a powerful webserver written in Go which provide a reverse proxy feature. In the example below, we'll use self-signed certificate. Follow the official documentation if you want to configure it to use an ACME like \"Let's Encrypt\" instead. Place your certificate and key file in the folder docker/certs . E.g: docker \u251c\u2500\u2500 Caddyfile \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 squest.crt \u2502 \u2514\u2500\u2500 squest.key Update the docker/Caddyfile with the FQDN of your server. By default, the FQDN is set to squest.domain.local squest.domain.local { # This line should match the ALLOWED_HOSTS in your Squest environment reverse_proxy nginx:8080 encode gzip zstd tls /etc/ssl/private/squest.crt /etc/ssl/private/squest.key # or: # tls /etc/ssl/private/cert.pem log { level error } } Update the ALLOWED_HOSTS environment variable from the configuration file docker/environment_variables/squest.env to match your FQDN. ALLOWED_HOSTS = squest.domain.local Start docker compose with the TLS configuration: docker-compose -f docker-compose.yml -f tls.docker-compose.yml up The squest service is then reachable via HTTP and HTTPS standard ports (80/443). http://squest.domain.local https://squest.domain.local","title":"TLS using Caddy"},{"location":"installation/upgrade/","text":"Squest upgrade This documentation aims at explaining how to perform an upgrade of squest on new release. Note Read the changelog of the version before performing any update to know what are the breaking changes or specific requirements of the new release. Note We recommend performing a manual backup before any upgrade. See the dedicated backup doc Stop all containers that use the Squest image docker-compose kill django celery-worker celery-beat Starting from here, the maintenance page should appear automatically in place of the Squest app. Pull the new image docker pull quay.io/hewlettpackardenterprise/squest:<version> E.g docker pull quay.io/hewlettpackardenterprise/squest:latest Start back containers docker-compose start django celery-worker celery-beat","title":"Upgrade"},{"location":"installation/upgrade/#squest-upgrade","text":"This documentation aims at explaining how to perform an upgrade of squest on new release. Note Read the changelog of the version before performing any update to know what are the breaking changes or specific requirements of the new release. Note We recommend performing a manual backup before any upgrade. See the dedicated backup doc Stop all containers that use the Squest image docker-compose kill django celery-worker celery-beat Starting from here, the maintenance page should appear automatically in place of the Squest app. Pull the new image docker pull quay.io/hewlettpackardenterprise/squest:<version> E.g docker pull quay.io/hewlettpackardenterprise/squest:latest Start back containers docker-compose start django celery-worker celery-beat","title":"Squest upgrade"},{"location":"manual/api/","text":"REST API Authentication Squest API allows tokens and session authentication. The API token management is available in the Tokens section of your profile page. A token is a unique identifier mapped to a Squest user account. Each user may have one or more tokens which can be used for authentication when making REST API requests. A token can have an expiration date to grant temporary access to an external client. Usage example with curl export SQUEST_TOKEN = d97ebdbeccf5fc3fba740e8e89048e3d453bd729 curl -X GET http://127.0.0.1:8000/api/resource_tracker/resource_group/ \\ -H \"Authorization: Bearer $SQUEST_TOKEN \" Usage example in Ansible URI module: - name : Get info from squest hosts : localhost connection : local gather_facts : false vars : squest_api : \"http://127.0.0.1:8000/api/\" squest_token : d97ebdbeccf5fc3fba740e8e89048e3d453bd729 squest_bearer_token : \"Bearer {{ squest_token }}\" tasks : - name : Get all resource group uri : url : \"{{ squest_api }}resource_tracker/resource_group/\" headers : Authorization : \"{{ squest_bearer_token }}\" method : GET status_code : 200 body_format : json register : output - debug : var : output API documentation The API documentation is available on the URL \"/swagger\" of your Squest instance. E.g: http://192.168.58.128/swagger/","title":"REST API"},{"location":"manual/api/#rest-api","text":"","title":"REST API"},{"location":"manual/api/#authentication","text":"Squest API allows tokens and session authentication. The API token management is available in the Tokens section of your profile page. A token is a unique identifier mapped to a Squest user account. Each user may have one or more tokens which can be used for authentication when making REST API requests. A token can have an expiration date to grant temporary access to an external client. Usage example with curl export SQUEST_TOKEN = d97ebdbeccf5fc3fba740e8e89048e3d453bd729 curl -X GET http://127.0.0.1:8000/api/resource_tracker/resource_group/ \\ -H \"Authorization: Bearer $SQUEST_TOKEN \" Usage example in Ansible URI module: - name : Get info from squest hosts : localhost connection : local gather_facts : false vars : squest_api : \"http://127.0.0.1:8000/api/\" squest_token : d97ebdbeccf5fc3fba740e8e89048e3d453bd729 squest_bearer_token : \"Bearer {{ squest_token }}\" tasks : - name : Get all resource group uri : url : \"{{ squest_api }}resource_tracker/resource_group/\" headers : Authorization : \"{{ squest_bearer_token }}\" method : GET status_code : 200 body_format : json register : output - debug : var : output","title":"Authentication"},{"location":"manual/api/#api-documentation","text":"The API documentation is available on the URL \"/swagger\" of your Squest instance. E.g: http://192.168.58.128/swagger/","title":"API documentation"},{"location":"manual/billing_groups/","text":"Billing groups Billing groups are linked to users and allow Squest administrator to visualize who is consuming what. Note Billing groups is an optional feature. Create billing groups As an administrator you can create multiple billing group and place user in them. A user can belong to multiple billing group. Configure billing group in the service catalog By default, Squest services are not linked to any billing group. Administrators can configure how the billing of the service will be predefined. Through the service form, choices are: Administrator defined billing group User defined billing group: From his billing group From all billing group Administrator defined billing group Administrators select a fixed billing group (can be none). Each created instance will be linked to this billing group. Administrators can also hide the billing from the end users. The billing group will not be asked neither shown in the end user form when requesting the service. User defined billing group From his billing group Administrators let the end user choose from his billing group when he sends an instance request. Note Users without billing groups can not request this service. From all billing group Administrators let the end user choose from all available billing group when he requests a service. Quota Quota can be used to: Track consumption of resources per billing group Create an auto approval request process by using them through global hooks and the squest API Quota and attributes A quota is a group of resource group attributes that you consider being of the same type. Let's take an example. We do provide 3 services through the catalog: Bare metal server Virtual machines in a VMware hypervisor K8S namespaces in a shared K8S cluster Each service create a resources in 3 different resource group of the resource tracking. The simplified graph view of resource group would be the following: Note All attributes are not represented. For a complete example of resource group and resource pool attribute link refer to the resource tracking doc No matter what service will be provisioned from the service catalog, we want to track for example our global consumption of CPU. The global CPU is actually linkable to: the CPU attribute of the \"Customer servers\" resource group the vCPU attribute of the \"Customer VMs\" resource group the request.cpu attribute of the \"K8S namespaces\" resource group So we would create a quota named \"CPU\" and link it to all resource group attribute definition Link quota to billing groups Once quota are defined, you can link them to each billing group and set a limit . Squest will then calculate the consumption by retrieving each instance that are linked to tracked resource attributes. Note The limit does not block any request from the service catalog. This field is used to give the administrator information about consumptions. The total consumption can exceed limit if the administrator provision services anyway. Consumed quota and limits are exposed as Prometheus metrics . You can then create your own Grafana dashboard from them to visualize percentage consumption of each attribute per billing group.","title":"Billing groups"},{"location":"manual/billing_groups/#billing-groups","text":"Billing groups are linked to users and allow Squest administrator to visualize who is consuming what. Note Billing groups is an optional feature.","title":"Billing groups"},{"location":"manual/billing_groups/#create-billing-groups","text":"As an administrator you can create multiple billing group and place user in them. A user can belong to multiple billing group.","title":"Create billing groups"},{"location":"manual/billing_groups/#configure-billing-group-in-the-service-catalog","text":"By default, Squest services are not linked to any billing group. Administrators can configure how the billing of the service will be predefined. Through the service form, choices are: Administrator defined billing group User defined billing group: From his billing group From all billing group","title":"Configure billing group in the service catalog"},{"location":"manual/billing_groups/#administrator-defined-billing-group","text":"Administrators select a fixed billing group (can be none). Each created instance will be linked to this billing group. Administrators can also hide the billing from the end users. The billing group will not be asked neither shown in the end user form when requesting the service.","title":"Administrator defined billing group"},{"location":"manual/billing_groups/#user-defined-billing-group","text":"","title":"User defined billing group"},{"location":"manual/billing_groups/#from-his-billing-group","text":"Administrators let the end user choose from his billing group when he sends an instance request. Note Users without billing groups can not request this service.","title":"From his billing group"},{"location":"manual/billing_groups/#from-all-billing-group","text":"Administrators let the end user choose from all available billing group when he requests a service.","title":"From all billing group"},{"location":"manual/billing_groups/#quota","text":"Quota can be used to: Track consumption of resources per billing group Create an auto approval request process by using them through global hooks and the squest API","title":"Quota"},{"location":"manual/billing_groups/#quota-and-attributes","text":"A quota is a group of resource group attributes that you consider being of the same type. Let's take an example. We do provide 3 services through the catalog: Bare metal server Virtual machines in a VMware hypervisor K8S namespaces in a shared K8S cluster Each service create a resources in 3 different resource group of the resource tracking. The simplified graph view of resource group would be the following: Note All attributes are not represented. For a complete example of resource group and resource pool attribute link refer to the resource tracking doc No matter what service will be provisioned from the service catalog, we want to track for example our global consumption of CPU. The global CPU is actually linkable to: the CPU attribute of the \"Customer servers\" resource group the vCPU attribute of the \"Customer VMs\" resource group the request.cpu attribute of the \"K8S namespaces\" resource group So we would create a quota named \"CPU\" and link it to all resource group attribute definition","title":"Quota and attributes"},{"location":"manual/billing_groups/#link-quota-to-billing-groups","text":"Once quota are defined, you can link them to each billing group and set a limit . Squest will then calculate the consumption by retrieving each instance that are linked to tracked resource attributes. Note The limit does not block any request from the service catalog. This field is used to give the administrator information about consumptions. The total consumption can exceed limit if the administrator provision services anyway. Consumed quota and limits are exposed as Prometheus metrics . You can then create your own Grafana dashboard from them to visualize percentage consumption of each attribute per billing group.","title":"Link quota to billing groups"},{"location":"manual/metrics/","text":"Prometheus metrics Squest supports optionally exposing native Prometheus metrics from the application. Prometheus is a popular time series metric platform used for monitoring. Squest exposes metrics at the /metrics HTTP endpoint, e.g. https://squest.domain.local/metrics. Squest config Metrics page is disabled by default. Update your docker/environment_variables/squest.env to enable metrics. METRICS_ENABLED = True METRICS_PASSWORD_PROTECTED = True METRICS_AUTHORIZATION_USERNAME = admin METRICS_AUTHORIZATION_PASSWORD = my_secret_password Prometheus config Here is an example of prometheus configuration you can use to scrape squest metrics scrape_configs : - job_name : 'squest' scrape_interval : 30s metrics_path : '/metrics/' static_configs : - targets : [ 'squest.domain.local' ] scheme : http basic_auth : username : admin password : my_secret_password Exported metrics squest_instance_per_service_total Expose the total number of instance per service. Labels: ['service'] E.g: squest_instance_per_service_total{service=\"Kubernetes\"} 5.0 squest_instance_per_service_total{service=\"Openshift\"} 11.0 squest_instance_per_service_total{service=\"VMWare\"} 14.0 squest_instance_per_state_total Expose the total number of instance per state. Labels: ['state'] E.g: squest_instance_per_state_total{state=\"AVAILABLE\"} 2.0 squest_instance_per_state_total{state=\"PENDING\"} 28.0 squest_request_per_state_total Expose the total number of request per state. Labels: ['state'] E.g: squest_request_per_state_total{state=\"ACCEPTED\"} 4.0 squest_request_per_state_total{state=\"CANCELED\"} 3.0 squest_request_per_state_total{state=\"COMPLETE\"} 5.0 squest_request_per_state_total{state=\"FAILED\"} 4.0 squest_request_per_state_total{state=\"NEED_INFO\"} 2.0 squest_request_per_state_total{state=\"PROCESSING\"} 3.0 squest_request_per_state_total{state=\"REJECTED\"} 5.0 squest_request_per_state_total{state=\"SUBMITTED\"} 4.00 squest_instance_total Total number of instance in squest Labels: ['service', 'state', 'billing_group'] E.g: squest_instance_total{billing_group=\"Orchestration\",service=\"VMWare\",state=\"AVAILABLE\"} 1.0 squest_instance_total{billing_group=\"Assurance\",service=\"VMWare\",state=\"AVAILABLE\"} 1.0 squest_instance_total{billing_group=\"Orchestration\",service=\"VMWare\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"5G\",service=\"VMWare\",state=\"PENDING\"} 6.0 squest_instance_total{billing_group=\"Assurance\",service=\"VMWare\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"Assurance\",service=\"Openshift\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"5G\",service=\"Openshift\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"Orchestration\",service=\"Openshift\",state=\"PENDING\"} 4.0 squest_instance_total{billing_group=\"Orchestration\",service=\"Kubernetes\",state=\"PENDING\"} 1.0 squest_instance_total{billing_group=\"5G\",service=\"Kubernetes\",state=\"PENDING\"} 2.0 squest_instance_total{billing_group=\"Assurance\",service=\"Kubernetes\",state=\"PENDING\"} 2.0 squest_instance_total{billing_group=\"None\",service=\"Openshift\",state=\"PENDING\"} 1.0 squest_request_total Total number of request in squest Labels: ['service', 'state'] E.g: squest_request_total{service=\"VMWare\",state=\"COMPLETE\"} 3.0 squest_request_total{service=\"VMWare\",state=\"PROCESSING\"} 2.0 squest_request_total{service=\"VMWare\",state=\"ACCEPTED\"} 2.0 squest_request_total{service=\"VMWare\",state=\"NEED_INFO\"} 1.0 squest_request_total{service=\"VMWare\",state=\"REJECTED\"} 4.0 squest_request_total{service=\"VMWare\",state=\"SUBMITTED\"} 1.0 squest_request_total{service=\"VMWare\",state=\"FAILED\"} 1.0 squest_request_total{service=\"Openshift\",state=\"REJECTED\"} 1.0 squest_request_total{service=\"Openshift\",state=\"CANCELED\"} 2.0 squest_request_total{service=\"Openshift\",state=\"FAILED\"} 3.0 squest_request_total{service=\"Openshift\",state=\"COMPLETE\"} 1.0 squest_request_total{service=\"Openshift\",state=\"SUBMITTED\"} 2.0 squest_request_total{service=\"Openshift\",state=\"ACCEPTED\"} 2.0 squest_request_total{service=\"Kubernetes\",state=\"SUBMITTED\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"COMPLETE\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"CANCELED\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"PROCESSING\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"NEED_INFO\"} 1.0 squest_support_total Total number of support Labels: ['state'] E.g: squest_support_total{service=\"VMWare\",state=\"CLOSED\"} 2.0 squest_support_total{service=\"VMWare\",state=\"OPENED\"} 1.0 squest_user_total Total number of user Labels: ['is_superuser'] E.g: squest_user_total{is_superuser=\"true\"} 1.0 squest_user_total{is_superuser=\"false\"} 6.0 squest_team_total Total number of team E.g: squest_team_total 3.0 squest_billing_group_total Total number of team E.g: squest_billing_group_total 3.0 squest_quota_consumed Consumption of quota per billing group and attribute E.g: squest_quota_consumed{billing_group=\"5G\",quota_attribute=\"CPU\"} 22.0 squest_quota_consumed{billing_group=\"5G\",quota_attribute=\"Memory\"} 45.0 squest_quota_consumed{billing_group=\"Assurance\",quota_attribute=\"CPU\"} 20.0 squest_quota_consumed{billing_group=\"Assurance\",quota_attribute=\"Memory\"} 23.0 squest_quota_limit Limit of quota per billing group and attribute squest_quota_limit{billing_group=\"5G\",quota_attribute=\"CPU\"} 100.0 squest_quota_limit{billing_group=\"5G\",quota_attribute=\"Memory\"} 50.0 squest_quota_limit{billing_group=\"Assurance\",quota_attribute=\"CPU\"} 45.0 squest_quota_limit{billing_group=\"Assurance\",quota_attribute=\"Memory\"} 12.0) A percentage of consumption can be calculated by using squest_quota_consumed and squest_quota_limit . PromQL example: round((squest_quota_consumed / squest_quota_limit) * 100)","title":"Prometheus metrics"},{"location":"manual/metrics/#prometheus-metrics","text":"Squest supports optionally exposing native Prometheus metrics from the application. Prometheus is a popular time series metric platform used for monitoring. Squest exposes metrics at the /metrics HTTP endpoint, e.g. https://squest.domain.local/metrics.","title":"Prometheus metrics"},{"location":"manual/metrics/#squest-config","text":"Metrics page is disabled by default. Update your docker/environment_variables/squest.env to enable metrics. METRICS_ENABLED = True METRICS_PASSWORD_PROTECTED = True METRICS_AUTHORIZATION_USERNAME = admin METRICS_AUTHORIZATION_PASSWORD = my_secret_password","title":"Squest config"},{"location":"manual/metrics/#prometheus-config","text":"Here is an example of prometheus configuration you can use to scrape squest metrics scrape_configs : - job_name : 'squest' scrape_interval : 30s metrics_path : '/metrics/' static_configs : - targets : [ 'squest.domain.local' ] scheme : http basic_auth : username : admin password : my_secret_password","title":"Prometheus config"},{"location":"manual/metrics/#exported-metrics","text":"","title":"Exported metrics"},{"location":"manual/metrics/#squest_instance_per_service_total","text":"Expose the total number of instance per service. Labels: ['service'] E.g: squest_instance_per_service_total{service=\"Kubernetes\"} 5.0 squest_instance_per_service_total{service=\"Openshift\"} 11.0 squest_instance_per_service_total{service=\"VMWare\"} 14.0","title":"squest_instance_per_service_total"},{"location":"manual/metrics/#squest_instance_per_state_total","text":"Expose the total number of instance per state. Labels: ['state'] E.g: squest_instance_per_state_total{state=\"AVAILABLE\"} 2.0 squest_instance_per_state_total{state=\"PENDING\"} 28.0","title":"squest_instance_per_state_total"},{"location":"manual/metrics/#squest_request_per_state_total","text":"Expose the total number of request per state. Labels: ['state'] E.g: squest_request_per_state_total{state=\"ACCEPTED\"} 4.0 squest_request_per_state_total{state=\"CANCELED\"} 3.0 squest_request_per_state_total{state=\"COMPLETE\"} 5.0 squest_request_per_state_total{state=\"FAILED\"} 4.0 squest_request_per_state_total{state=\"NEED_INFO\"} 2.0 squest_request_per_state_total{state=\"PROCESSING\"} 3.0 squest_request_per_state_total{state=\"REJECTED\"} 5.0 squest_request_per_state_total{state=\"SUBMITTED\"} 4.00","title":"squest_request_per_state_total"},{"location":"manual/metrics/#squest_instance_total","text":"Total number of instance in squest Labels: ['service', 'state', 'billing_group'] E.g: squest_instance_total{billing_group=\"Orchestration\",service=\"VMWare\",state=\"AVAILABLE\"} 1.0 squest_instance_total{billing_group=\"Assurance\",service=\"VMWare\",state=\"AVAILABLE\"} 1.0 squest_instance_total{billing_group=\"Orchestration\",service=\"VMWare\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"5G\",service=\"VMWare\",state=\"PENDING\"} 6.0 squest_instance_total{billing_group=\"Assurance\",service=\"VMWare\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"Assurance\",service=\"Openshift\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"5G\",service=\"Openshift\",state=\"PENDING\"} 3.0 squest_instance_total{billing_group=\"Orchestration\",service=\"Openshift\",state=\"PENDING\"} 4.0 squest_instance_total{billing_group=\"Orchestration\",service=\"Kubernetes\",state=\"PENDING\"} 1.0 squest_instance_total{billing_group=\"5G\",service=\"Kubernetes\",state=\"PENDING\"} 2.0 squest_instance_total{billing_group=\"Assurance\",service=\"Kubernetes\",state=\"PENDING\"} 2.0 squest_instance_total{billing_group=\"None\",service=\"Openshift\",state=\"PENDING\"} 1.0","title":"squest_instance_total"},{"location":"manual/metrics/#squest_request_total","text":"Total number of request in squest Labels: ['service', 'state'] E.g: squest_request_total{service=\"VMWare\",state=\"COMPLETE\"} 3.0 squest_request_total{service=\"VMWare\",state=\"PROCESSING\"} 2.0 squest_request_total{service=\"VMWare\",state=\"ACCEPTED\"} 2.0 squest_request_total{service=\"VMWare\",state=\"NEED_INFO\"} 1.0 squest_request_total{service=\"VMWare\",state=\"REJECTED\"} 4.0 squest_request_total{service=\"VMWare\",state=\"SUBMITTED\"} 1.0 squest_request_total{service=\"VMWare\",state=\"FAILED\"} 1.0 squest_request_total{service=\"Openshift\",state=\"REJECTED\"} 1.0 squest_request_total{service=\"Openshift\",state=\"CANCELED\"} 2.0 squest_request_total{service=\"Openshift\",state=\"FAILED\"} 3.0 squest_request_total{service=\"Openshift\",state=\"COMPLETE\"} 1.0 squest_request_total{service=\"Openshift\",state=\"SUBMITTED\"} 2.0 squest_request_total{service=\"Openshift\",state=\"ACCEPTED\"} 2.0 squest_request_total{service=\"Kubernetes\",state=\"SUBMITTED\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"COMPLETE\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"CANCELED\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"PROCESSING\"} 1.0 squest_request_total{service=\"Kubernetes\",state=\"NEED_INFO\"} 1.0","title":"squest_request_total"},{"location":"manual/metrics/#squest_support_total","text":"Total number of support Labels: ['state'] E.g: squest_support_total{service=\"VMWare\",state=\"CLOSED\"} 2.0 squest_support_total{service=\"VMWare\",state=\"OPENED\"} 1.0","title":"squest_support_total"},{"location":"manual/metrics/#squest_user_total","text":"Total number of user Labels: ['is_superuser'] E.g: squest_user_total{is_superuser=\"true\"} 1.0 squest_user_total{is_superuser=\"false\"} 6.0","title":"squest_user_total"},{"location":"manual/metrics/#squest_team_total","text":"Total number of team E.g: squest_team_total 3.0","title":"squest_team_total"},{"location":"manual/metrics/#squest_billing_group_total","text":"Total number of team E.g: squest_billing_group_total 3.0","title":"squest_billing_group_total"},{"location":"manual/metrics/#squest_quota_consumed","text":"Consumption of quota per billing group and attribute E.g: squest_quota_consumed{billing_group=\"5G\",quota_attribute=\"CPU\"} 22.0 squest_quota_consumed{billing_group=\"5G\",quota_attribute=\"Memory\"} 45.0 squest_quota_consumed{billing_group=\"Assurance\",quota_attribute=\"CPU\"} 20.0 squest_quota_consumed{billing_group=\"Assurance\",quota_attribute=\"Memory\"} 23.0","title":"squest_quota_consumed"},{"location":"manual/metrics/#squest_quota_limit","text":"Limit of quota per billing group and attribute squest_quota_limit{billing_group=\"5G\",quota_attribute=\"CPU\"} 100.0 squest_quota_limit{billing_group=\"5G\",quota_attribute=\"Memory\"} 50.0 squest_quota_limit{billing_group=\"Assurance\",quota_attribute=\"CPU\"} 45.0 squest_quota_limit{billing_group=\"Assurance\",quota_attribute=\"Memory\"} 12.0) A percentage of consumption can be calculated by using squest_quota_consumed and squest_quota_limit . PromQL example: round((squest_quota_consumed / squest_quota_limit) * 100)","title":"squest_quota_limit"},{"location":"manual/notifications/","text":"Notifications Squest current notification system only support emails. Enable or disable notifications By default, notifications are enabled. You can disable all notifications from your profile page by accessing the user details page in the top right corner of the Squest application. Note Administrators will not receive any notification without subscribing to some services. See below. Subscribe to service notifications (admin) By default, administrator are not linked to any service. By so, they don't receive any email notification of new requests, supports or comments. As an admin, you need to select the services you want to follow in the notification panel configuration of your user profile page.","title":"Notifications"},{"location":"manual/notifications/#notifications","text":"Squest current notification system only support emails.","title":"Notifications"},{"location":"manual/notifications/#enable-or-disable-notifications","text":"By default, notifications are enabled. You can disable all notifications from your profile page by accessing the user details page in the top right corner of the Squest application. Note Administrators will not receive any notification without subscribing to some services. See below.","title":"Enable or disable notifications"},{"location":"manual/notifications/#subscribe-to-service-notifications-admin","text":"By default, administrator are not linked to any service. By so, they don't receive any email notification of new requests, supports or comments. As an admin, you need to select the services you want to follow in the notification panel configuration of your user profile page.","title":"Subscribe to service notifications (admin)"},{"location":"manual/resource_tracking/","text":"Resource tracking Nowadays, IT infrastructures are composed of multiple layers. Physical servers, virtual machines, containers, storage,... Each layer is consumer or a producer of resources of another layer. As an IT administrator, we need to monitor resource consumption of a top layers to be sure that we can provide services on underlying layers. The resource tracking feature allows to monitor reserved resources and highlight available resource in an infrastructure. Note This feature is not a real time monitoring. It does not connect to you infrastructure to check the real consumption but help to follow what resources have been reserved to avoid overallocation when accepting new request from the service catalog. Concept To introduce the concept of resources, lets take the example of a virtualization stack like a VMware vCenter. The stack is composed of physical servers that produce resources like CPU and memory to the hypervisor. Then we can create VMs that will consume those resources as virtual CPU( vCPU ) and memory . The hypervisor is then a resource pool with producers and consumers of resources. If we want to add more VMs that consume resources from the hypervisor resource pool, we need to be sure we have enough physical servers that produce into it. This stack could be drawn like the following with resource group (blue), resource pool (orange) and resources (green): In this example, we have: two resources of the same kind(server) that produce resources into the resource pool hypervisor two resources of the same kind(VM) that consume resources from the resource pool hypervisor As server resources are of the same kind and produce in the same pool on the same attributes , they belong to the same resource group servers . As VM resources are of the same kind and consume in the same pool on the same attributes , they belong to the same resource group VMs . The previous drawing could be then simplified by just showing resource groups and resource pool : Generic objects Resource pool A resource pool is a generic object composed by attributes . Resource pool attributes have producers and consumers which are attributes from resource groups . The sum of all resource group attributes that produce in the same resource pool attribute give a total produced . The sum of all resource group attributes that consume in the same resource pool attribute give a total consumed . The difference between total produced and total consumed gives the amount of available resources for a particular resource pool attribute . Resource group A resource group is a definition of a resource of same kind, composed of attributes that may produce or consume from a particular resource pool . Example of resource group: Bare metal servers Physical disks VMware VMs K8S namespaces Openshift projects Openstack tenants Resource groups also have text attributes to provide some additional information. Resource A resource is an instance of a resource group definition. Resources can be created, updated or deleted from the Squest UI or API. Updating resources in a resource group impact the total amount of produced or consumed resource on pool's attributes. Multiple layer example In this example we do track the consumption of an orchestrator of container like Kubernetes or Openshift. Namespaces (or projects in Openshift world) are a way to divide cluster resources between multiple users by using resource quota. Openshift and Kubernetes frameworks are commonly deployed in a virtual machines. So we retrieve layers from previous example with bare metal servers that produce resources in an hypervisor. Orchestrators are usually composed of 2 kind of node: Masters and Workers. Master VMs are used by the infrastructure itself and workers for user's workloads, aka namespaces. As namespaces are only executed in \"worker\" nodes, we need to declare 2 different resource group : Master VMs Worker VMs The aggregation of resources of all workers compose the resource pool of available resources that the namespaces resource group will consume. The complete resource tracking definition would look like the following: With this definition, we are able to determine there is enough available resources in pools to handle underlying objects. Adding a new namespace in the last resource group namespaces will generate more consumption on the K8S resource pool . If this last pool is lacking of resources, adding more worker node in the worker VMs resource group will be required, generating consumption on upper layers and so on... Link service catalog instances to resources Resources can be created from the API. It allows to create automatically a new resource in a resource group when something is provisioned from the service catalog. In the example below, the playbook executed in Tower/AWX would have created a VM. At the end of the process we call the squest API to instantiate a resource in the right resource group to reflect the consumption. We link as well the pending instance(given by squest.instance.id ) to this resource via the flag service_catalog_instance . - name : Add resource in resource group example hosts : localhost connection : local gather_facts : false vars : squest_token : 48c67f9c2429f2d3a1ee0e47daa00ffeef4fe744 squest_bearer_token : \"Bearer {{ squest_token }}\" squest_api : \"http://127.0.0.1:8000/api/\" resource_group_vm_id : 8 squest : # this would be the sent data from squest as extra vars instance : id : 8 name : test service : 1 spec : { } state : PROVISIONING vm_name : \"test-vm\" vm_vcpu : 4 vm_memory : 16 desc : \"My description\" tasks : - name : Print info sent by Squest debug : var : squest # ----------------------- # PLACE HERE ALL THE MAGIC TO CREATE THE RESOURCE # ----------------------- - name : Create a resource in squest uri : url : \"{{ squest_api }}resource_tracker/resource_group/{{ resource_group_vm_id }}/resources/\" headers : Authorization : \"{{ squest_bearer_token }}\" method : POST body : name : \"{{ vm_name }}\" service_catalog_instance : \"{{ squest['instance']['id'] }}\" attributes : - name : \"vCPU\" value : \"{{ vm_vcpu }}\" - name : \"Memory\" value : \"{{ vm_memory }}\" text_attributes : - name : \"Description\" value : \"{{ desc }}\" status_code : 201 body_format : json Tags Tags are words that are attached to objects, such as Resource Pool or Resource Group. Tags are intended to be used to specify identifying objects that are meaningful and relevant to users. Tags can be used to organize and select subsets of objects. Tags can be attached to objects at creation time and subsequently added and modified at any time. How to add multiple tags If the input doesn't contain any commas or double quotes, it is simply treated as a space-delimited list of tag names. If the input does contain either of these characters: Groups of characters which appear between double quotes take precedence as multi-word tags (so double quoted tag names may contain commas). An unclosed double quote will be ignored. Otherwise, if there are any unquoted commas in the input, it will be treated as comma-delimited. If not, it will be treated as space-delimited. Examples: Tag input string Resulting tags Notes apple ball cat [\"apple\", \"ball\", \"cat\"] No commas, so space delimited apple, ball cat [\"apple\", \"ball cat\"] Comma present, so comma delimited \"apple, ball\" cat dog [\"apple, ball\", \"cat\", \"dog\"] All commas are quoted, so space delimited \"apple, ball\", cat dog [\"apple, ball\", \"cat dog\"] Contains an unquoted comma, so comma delimited apple \"ball cat\" dog [\"apple\", \"ball cat\", \"dog\"] No commas, so space delimited \"apple\" \"ball dog [\"apple\", \"ball\", \"dog\"] Unclosed double quote is ignored Over commitment Over commitment is available on all resource pool attributes for consumers and producers. The over commitment allows you to specify whether resource pools produce/consume more or less than expected. The most common case is CPU/vCPU: If a host has 28 core processors and hyperthreading is enabled, that host will produce 56 vCPUs (28 cores x 2 threads per core) then you can configure the over commitment on the CPU produced attribute to 2 to match this behavior.","title":"Resource tracking"},{"location":"manual/resource_tracking/#resource-tracking","text":"Nowadays, IT infrastructures are composed of multiple layers. Physical servers, virtual machines, containers, storage,... Each layer is consumer or a producer of resources of another layer. As an IT administrator, we need to monitor resource consumption of a top layers to be sure that we can provide services on underlying layers. The resource tracking feature allows to monitor reserved resources and highlight available resource in an infrastructure. Note This feature is not a real time monitoring. It does not connect to you infrastructure to check the real consumption but help to follow what resources have been reserved to avoid overallocation when accepting new request from the service catalog.","title":"Resource tracking"},{"location":"manual/resource_tracking/#concept","text":"To introduce the concept of resources, lets take the example of a virtualization stack like a VMware vCenter. The stack is composed of physical servers that produce resources like CPU and memory to the hypervisor. Then we can create VMs that will consume those resources as virtual CPU( vCPU ) and memory . The hypervisor is then a resource pool with producers and consumers of resources. If we want to add more VMs that consume resources from the hypervisor resource pool, we need to be sure we have enough physical servers that produce into it. This stack could be drawn like the following with resource group (blue), resource pool (orange) and resources (green): In this example, we have: two resources of the same kind(server) that produce resources into the resource pool hypervisor two resources of the same kind(VM) that consume resources from the resource pool hypervisor As server resources are of the same kind and produce in the same pool on the same attributes , they belong to the same resource group servers . As VM resources are of the same kind and consume in the same pool on the same attributes , they belong to the same resource group VMs . The previous drawing could be then simplified by just showing resource groups and resource pool :","title":"Concept"},{"location":"manual/resource_tracking/#generic-objects","text":"","title":"Generic objects"},{"location":"manual/resource_tracking/#resource-pool","text":"A resource pool is a generic object composed by attributes . Resource pool attributes have producers and consumers which are attributes from resource groups . The sum of all resource group attributes that produce in the same resource pool attribute give a total produced . The sum of all resource group attributes that consume in the same resource pool attribute give a total consumed . The difference between total produced and total consumed gives the amount of available resources for a particular resource pool attribute .","title":"Resource pool"},{"location":"manual/resource_tracking/#resource-group","text":"A resource group is a definition of a resource of same kind, composed of attributes that may produce or consume from a particular resource pool . Example of resource group: Bare metal servers Physical disks VMware VMs K8S namespaces Openshift projects Openstack tenants Resource groups also have text attributes to provide some additional information.","title":"Resource group"},{"location":"manual/resource_tracking/#resource","text":"A resource is an instance of a resource group definition. Resources can be created, updated or deleted from the Squest UI or API. Updating resources in a resource group impact the total amount of produced or consumed resource on pool's attributes.","title":"Resource"},{"location":"manual/resource_tracking/#multiple-layer-example","text":"In this example we do track the consumption of an orchestrator of container like Kubernetes or Openshift. Namespaces (or projects in Openshift world) are a way to divide cluster resources between multiple users by using resource quota. Openshift and Kubernetes frameworks are commonly deployed in a virtual machines. So we retrieve layers from previous example with bare metal servers that produce resources in an hypervisor. Orchestrators are usually composed of 2 kind of node: Masters and Workers. Master VMs are used by the infrastructure itself and workers for user's workloads, aka namespaces. As namespaces are only executed in \"worker\" nodes, we need to declare 2 different resource group : Master VMs Worker VMs The aggregation of resources of all workers compose the resource pool of available resources that the namespaces resource group will consume. The complete resource tracking definition would look like the following: With this definition, we are able to determine there is enough available resources in pools to handle underlying objects. Adding a new namespace in the last resource group namespaces will generate more consumption on the K8S resource pool . If this last pool is lacking of resources, adding more worker node in the worker VMs resource group will be required, generating consumption on upper layers and so on...","title":"Multiple layer example"},{"location":"manual/resource_tracking/#link-service-catalog-instances-to-resources","text":"Resources can be created from the API. It allows to create automatically a new resource in a resource group when something is provisioned from the service catalog. In the example below, the playbook executed in Tower/AWX would have created a VM. At the end of the process we call the squest API to instantiate a resource in the right resource group to reflect the consumption. We link as well the pending instance(given by squest.instance.id ) to this resource via the flag service_catalog_instance . - name : Add resource in resource group example hosts : localhost connection : local gather_facts : false vars : squest_token : 48c67f9c2429f2d3a1ee0e47daa00ffeef4fe744 squest_bearer_token : \"Bearer {{ squest_token }}\" squest_api : \"http://127.0.0.1:8000/api/\" resource_group_vm_id : 8 squest : # this would be the sent data from squest as extra vars instance : id : 8 name : test service : 1 spec : { } state : PROVISIONING vm_name : \"test-vm\" vm_vcpu : 4 vm_memory : 16 desc : \"My description\" tasks : - name : Print info sent by Squest debug : var : squest # ----------------------- # PLACE HERE ALL THE MAGIC TO CREATE THE RESOURCE # ----------------------- - name : Create a resource in squest uri : url : \"{{ squest_api }}resource_tracker/resource_group/{{ resource_group_vm_id }}/resources/\" headers : Authorization : \"{{ squest_bearer_token }}\" method : POST body : name : \"{{ vm_name }}\" service_catalog_instance : \"{{ squest['instance']['id'] }}\" attributes : - name : \"vCPU\" value : \"{{ vm_vcpu }}\" - name : \"Memory\" value : \"{{ vm_memory }}\" text_attributes : - name : \"Description\" value : \"{{ desc }}\" status_code : 201 body_format : json","title":"Link service catalog instances to resources"},{"location":"manual/resource_tracking/#tags","text":"Tags are words that are attached to objects, such as Resource Pool or Resource Group. Tags are intended to be used to specify identifying objects that are meaningful and relevant to users. Tags can be used to organize and select subsets of objects. Tags can be attached to objects at creation time and subsequently added and modified at any time.","title":"Tags"},{"location":"manual/resource_tracking/#how-to-add-multiple-tags","text":"If the input doesn't contain any commas or double quotes, it is simply treated as a space-delimited list of tag names. If the input does contain either of these characters: Groups of characters which appear between double quotes take precedence as multi-word tags (so double quoted tag names may contain commas). An unclosed double quote will be ignored. Otherwise, if there are any unquoted commas in the input, it will be treated as comma-delimited. If not, it will be treated as space-delimited. Examples: Tag input string Resulting tags Notes apple ball cat [\"apple\", \"ball\", \"cat\"] No commas, so space delimited apple, ball cat [\"apple\", \"ball cat\"] Comma present, so comma delimited \"apple, ball\" cat dog [\"apple, ball\", \"cat\", \"dog\"] All commas are quoted, so space delimited \"apple, ball\", cat dog [\"apple, ball\", \"cat dog\"] Contains an unquoted comma, so comma delimited apple \"ball cat\" dog [\"apple\", \"ball cat\", \"dog\"] No commas, so space delimited \"apple\" \"ball dog [\"apple\", \"ball\", \"dog\"] Unclosed double quote is ignored","title":"How to add multiple tags"},{"location":"manual/resource_tracking/#over-commitment","text":"Over commitment is available on all resource pool attributes for consumers and producers. The over commitment allows you to specify whether resource pools produce/consume more or less than expected. The most common case is CPU/vCPU: If a host has 28 core processors and hyperthreading is enabled, that host will produce 56 vCPUs (28 cores x 2 threads per core) then you can configure the over commitment on the CPU produced attribute to 2 to match this behavior.","title":"Over commitment"},{"location":"manual/service_catalog/","text":"Lifecycle management Populate the service catalog Once Squest is linked to a Tower/AWX server, \"services\" can be added into the catalog. A service is composed of operations that are pointers to \"job templates\" present in Tower/AWX. A service has at least one operation of type CREATE that allows to provision the resource. A service can have then multiple operation of type UPDATE and DELETE that allow to manage the lifecycle of instances that have been created via the CREATE operation. Provisioning a service When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : squest_host : http://squest.domain.local request : instance : id : 1 name : test service : 1 spec : file_name : foo.conf state : PROVISIONING spoc : 2 Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project) Playbook example In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_token : 48c67f9c2429f2d3a1ee0e47daa00ffeef4fe744 squest_bearer_token : \"Bearer {{ squest_token }}\" squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job template survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}service_catalog/instance/{{ squest['request']['instance']['id'] }}/\" # do not forget the last slash headers : Authorization : \"{{ squest_bearer_token }}\" method : PATCH body : spec : file_name : \"{{ file_name }}\" status_code : 200 body_format : json Day 2 operations Day 2 operations are operations that update or delete existing resources. Note By default, recent version of AWX/Tower drop extra variables that are not declared in the survey. To be able to receive Squest extra vars you need to enable \"Prompt on Launch\" in the \"Variables\" section of you job template. This correspond to the flag \"ask_variables_on_launch\" of the job_template model on the Tower/AWX API. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated Playbook example Example of extra vars sent by squest: squest : squest_host : http://squest.domain.local request : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf spoc : 2 state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job template survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['request']['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes Operation survey The survey of an operation can be edited to change the behavior of the generated form of a request. Note Surveys in Squest are actually surveys attached to each job templates in your Tower/AWX. Squest can only disable the ones that you don't want to be filled by your end users. Those fields, if declared as mandatory on Tower/AWX, will need to be filled anyway by the admin when approving a request. Enabled fields An enabled field is displayed into the end user survey. By default, all fields are enabled when creating a new operation. Note If the field is set as required into the Tower/AWX job template survey config then the administrator will have to fill it in any case during the review of the request. Default value When set, the default value is pre-filled into the final form. It takes precedence over the default value set in Tower/AWX job template survey config. Default value precedence: Default from Tower/AWX Default from Squest value User's input Admin's input Note When used with a 'multiple select' or 'multiple select multiple' type of field, the value need to be a valid one from the Tower/AWX survey field options. Hard coded string Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config My hard coded value Result My hard coded value With spec variable Jinja templating can be used to load a value with the content of the instance spec or user spec. Use variables spec and/or user_spec like with Ansible. Note The spec and user_spec variables are only usable on Update or Delete operations as the pending instance does not contain any spec before its provisioning. Note If the given variable key doesn't exist, the default value will be set to an empty string. Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config {{ spec.os }} Result linux String and spec variable Like with Ansible, a string can be concatenated to a spec variable. Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config My hard coded value with {{ spec.os }} Result My hard coded value with linux Spec dict access Instance JSON spec { \"spec\" : { \"os\" : { \"linux\" : \"ubuntu\" } }, \"user_spec\" : {} } Default config {{ spec.os['linux'] }} Result ubuntu Spec list access Instance JSON spec { \"spec\" : { \"os\" : [ \"linux\" , \"windows\" ] }, \"user_spec\" : {} } Default config {{ spec.os[1] }} Result windows Jinja filters Jinja filters can also be used to transform spec variables. For example, the 'multiple select multiple' field type requires a list of string separated with a carriage return marker ( \\n ). Instance JSON spec { \"spec\" : { \"os\" : [ \"linux\" , \"windows\" ] }, \"user_spec\" : {} } Default config {{ spec.os | join('\\n') }} Result linux\\nwindows","title":"Service catalog"},{"location":"manual/service_catalog/#lifecycle-management","text":"","title":"Lifecycle management"},{"location":"manual/service_catalog/#populate-the-service-catalog","text":"Once Squest is linked to a Tower/AWX server, \"services\" can be added into the catalog. A service is composed of operations that are pointers to \"job templates\" present in Tower/AWX. A service has at least one operation of type CREATE that allows to provision the resource. A service can have then multiple operation of type UPDATE and DELETE that allow to manage the lifecycle of instances that have been created via the CREATE operation.","title":"Populate the service catalog"},{"location":"manual/service_catalog/#provisioning-a-service","text":"When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : squest_host : http://squest.domain.local request : instance : id : 1 name : test service : 1 spec : file_name : foo.conf state : PROVISIONING spoc : 2 Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project)","title":"Provisioning a service"},{"location":"manual/service_catalog/#playbook-example","text":"In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_token : 48c67f9c2429f2d3a1ee0e47daa00ffeef4fe744 squest_bearer_token : \"Bearer {{ squest_token }}\" squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job template survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}service_catalog/instance/{{ squest['request']['instance']['id'] }}/\" # do not forget the last slash headers : Authorization : \"{{ squest_bearer_token }}\" method : PATCH body : spec : file_name : \"{{ file_name }}\" status_code : 200 body_format : json","title":"Playbook example"},{"location":"manual/service_catalog/#day-2-operations","text":"Day 2 operations are operations that update or delete existing resources. Note By default, recent version of AWX/Tower drop extra variables that are not declared in the survey. To be able to receive Squest extra vars you need to enable \"Prompt on Launch\" in the \"Variables\" section of you job template. This correspond to the flag \"ask_variables_on_launch\" of the job_template model on the Tower/AWX API. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated","title":"Day 2 operations"},{"location":"manual/service_catalog/#playbook-example_1","text":"Example of extra vars sent by squest: squest : squest_host : http://squest.domain.local request : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf spoc : 2 state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job template survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['request']['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes","title":"Playbook example"},{"location":"manual/service_catalog/#operation-survey","text":"The survey of an operation can be edited to change the behavior of the generated form of a request. Note Surveys in Squest are actually surveys attached to each job templates in your Tower/AWX. Squest can only disable the ones that you don't want to be filled by your end users. Those fields, if declared as mandatory on Tower/AWX, will need to be filled anyway by the admin when approving a request.","title":"Operation survey"},{"location":"manual/service_catalog/#enabled-fields","text":"An enabled field is displayed into the end user survey. By default, all fields are enabled when creating a new operation. Note If the field is set as required into the Tower/AWX job template survey config then the administrator will have to fill it in any case during the review of the request.","title":"Enabled fields"},{"location":"manual/service_catalog/#default-value","text":"When set, the default value is pre-filled into the final form. It takes precedence over the default value set in Tower/AWX job template survey config. Default value precedence: Default from Tower/AWX Default from Squest value User's input Admin's input Note When used with a 'multiple select' or 'multiple select multiple' type of field, the value need to be a valid one from the Tower/AWX survey field options.","title":"Default value"},{"location":"manual/service_catalog/#hard-coded-string","text":"Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config My hard coded value Result My hard coded value","title":"Hard coded string"},{"location":"manual/service_catalog/#with-spec-variable","text":"Jinja templating can be used to load a value with the content of the instance spec or user spec. Use variables spec and/or user_spec like with Ansible. Note The spec and user_spec variables are only usable on Update or Delete operations as the pending instance does not contain any spec before its provisioning. Note If the given variable key doesn't exist, the default value will be set to an empty string. Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config {{ spec.os }} Result linux","title":"With spec variable"},{"location":"manual/service_catalog/#string-and-spec-variable","text":"Like with Ansible, a string can be concatenated to a spec variable. Instance JSON spec { \"spec\" : { \"os\" : \"linux\" }, \"user_spec\" : {} } Default config My hard coded value with {{ spec.os }} Result My hard coded value with linux","title":"String and spec variable"},{"location":"manual/service_catalog/#spec-dict-access","text":"Instance JSON spec { \"spec\" : { \"os\" : { \"linux\" : \"ubuntu\" } }, \"user_spec\" : {} } Default config {{ spec.os['linux'] }} Result ubuntu","title":"Spec dict access"},{"location":"manual/service_catalog/#spec-list-access","text":"Instance JSON spec { \"spec\" : { \"os\" : [ \"linux\" , \"windows\" ] }, \"user_spec\" : {} } Default config {{ spec.os[1] }} Result windows","title":"Spec list access"},{"location":"manual/service_catalog/#jinja-filters","text":"Jinja filters can also be used to transform spec variables. For example, the 'multiple select multiple' field type requires a list of string separated with a carriage return marker ( \\n ). Instance JSON spec { \"spec\" : { \"os\" : [ \"linux\" , \"windows\" ] }, \"user_spec\" : {} } Default config {{ spec.os | join('\\n') }} Result linux\\nwindows","title":"Jinja filters"},{"location":"manual/settings/","text":"Settings Global Hooks Global hooks are a way to call a Tower/AWX job template following the new state of a Request or an Instance . For example, if you want to call a job template that performs an action everytime a Request switch to FAILED state. Form field: name: Name of your hook Model: Target model object that will be linked to the hook ( Request or an Instance ) State: State of the selected model . The hook will be triggered when an instance of the select model type will switch to this selected state Job template: The Tower/AWX job template to execute when an instance of the selected model reach the selected state Extra vars: extra variable as JSON to add to the selected job template States documentation: Available states for a Request . Available states for a Instance . Announcements Note Configure your time zone . Announcements allow Squest administrator to notify users. Announcements are displayed to end users in Dashboard page. Administrator defines beginning, end, title, message and type of announcement.","title":"Settings"},{"location":"manual/settings/#settings","text":"","title":"Settings"},{"location":"manual/settings/#global-hooks","text":"Global hooks are a way to call a Tower/AWX job template following the new state of a Request or an Instance . For example, if you want to call a job template that performs an action everytime a Request switch to FAILED state. Form field: name: Name of your hook Model: Target model object that will be linked to the hook ( Request or an Instance ) State: State of the selected model . The hook will be triggered when an instance of the select model type will switch to this selected state Job template: The Tower/AWX job template to execute when an instance of the selected model reach the selected state Extra vars: extra variable as JSON to add to the selected job template States documentation: Available states for a Request . Available states for a Instance .","title":"Global Hooks"},{"location":"manual/settings/#announcements","text":"Note Configure your time zone . Announcements allow Squest administrator to notify users. Announcements are displayed to end users in Dashboard page. Administrator defines beginning, end, title, message and type of announcement.","title":"Announcements"},{"location":"manual/teams/","text":"Teams Teams can be used to give a set of permissions on an object to a user or a group of users. This feature allows, for example, to share instances of provisioned services to multiple users so they can retrieve and manage request of those instances in their own Squest session. A team can be created by any logged user. The creator become the default administrator of the team and can then: link instances add users set permissions Note A team can have multiple admins but needs to have at least one admin Team roles Role Team permissions Administrator List members Manage members Link instances Assign permissions Member List members Role permissions in teams Role Instance permissions Request permissions Administrator Open support Request new operations List Update Delete List Cancel Comment Update Delete Operator Open support Request new operations List List Cancel Comment Reader List List","title":"Teams"},{"location":"manual/teams/#teams","text":"Teams can be used to give a set of permissions on an object to a user or a group of users. This feature allows, for example, to share instances of provisioned services to multiple users so they can retrieve and manage request of those instances in their own Squest session. A team can be created by any logged user. The creator become the default administrator of the team and can then: link instances add users set permissions Note A team can have multiple admins but needs to have at least one admin","title":"Teams"},{"location":"manual/teams/#team-roles","text":"Role Team permissions Administrator List members Manage members Link instances Assign permissions Member List members","title":"Team roles"},{"location":"manual/teams/#role-permissions-in-teams","text":"Role Instance permissions Request permissions Administrator Open support Request new operations List Update Delete List Cancel Comment Update Delete Operator Open support Request new operations List List Cancel Comment Reader List List","title":"Role permissions in teams"}]}