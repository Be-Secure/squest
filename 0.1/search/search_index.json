{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Squest Squest is a Web portal that allow to expose Ansible Tower/AWX based automation as a service. Features: Catalog of service that point to an Ansible Tower/AWX job template Admin approval on requests Instance lifecycle management (update, delete) Resource tracking Billing groups If you want an idea of what you can do with Squest, click on the image below","title":"Home"},{"location":"#squest","text":"Squest is a Web portal that allow to expose Ansible Tower/AWX based automation as a service. Features: Catalog of service that point to an Ansible Tower/AWX job template Admin approval on requests Instance lifecycle management (update, delete) Resource tracking Billing groups If you want an idea of what you can do with Squest, click on the image below","title":"Squest"},{"location":"deployment/","text":"Deploy Squest The current deployment is based on Docker compose for testing the tool. Pre-requisites: docker docker-compose To test the application, run the full env docker compose file docker-compose -f dev-env.docker-compose.yml -f full-env.docker-compose.yml up Then connect with your web browser to http://127.0.0.1:8000 The default admin account is admin // admin","title":"Deployment"},{"location":"deployment/#deploy-squest","text":"The current deployment is based on Docker compose for testing the tool. Pre-requisites: docker docker-compose To test the application, run the full env docker compose file docker-compose -f dev-env.docker-compose.yml -f full-env.docker-compose.yml up Then connect with your web browser to http://127.0.0.1:8000 The default admin account is admin // admin","title":"Deploy Squest"},{"location":"squest_settings/","text":"Configure settings Settings are placed into the squest/settings/development.py file which is a standard Django core settings file LDAP backend LDAP can be activated by setting the environment vairable LDAP_ENABLED to True or directly in the settings.py file: LDAP_ENABLED = True The configuration file need then to be created in Squest/ldap_config.py . The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/path/to/my/company-ca.crt\" AUTH_LDAP_CONNECTION_OPTIONS : { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_START_TLS : True AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" } Email Email settings are based on the django settings EMAIL_HOST = os . environ . get ( 'EMAIL_HOST' , None ) EMAIL_PORT = 25","title":"Squest"},{"location":"squest_settings/#configure-settings","text":"Settings are placed into the squest/settings/development.py file which is a standard Django core settings file","title":"Configure settings"},{"location":"squest_settings/#ldap-backend","text":"LDAP can be activated by setting the environment vairable LDAP_ENABLED to True or directly in the settings.py file: LDAP_ENABLED = True The configuration file need then to be created in Squest/ldap_config.py . The configuration is based on the Django plugin django-auth-ldap . You can follow the official documentation to know available configuration options. Example of ldap_config.py : import os import ldap from django_auth_ldap.config import LDAPSearch print ( \"LDAP config loaded\" ) # ----------------------- # LDAP auth backend # ----------------------- AUTH_LDAP_SERVER_URI = \"ldaps://ad.example.com\" AUTH_LDAP_BIND_DN = \"CN=my_app,OU=Service_Accounts,DC=example,DC=com\" AUTH_LDAP_BIND_PASSWORD = os . environ . get ( 'AUTH_LDAP_BIND_PASSWORD' , None ) AUTH_LDAP_USER_SEARCH = LDAPSearch ( \"OU=Service_Accounts,DC=example,DC=com\" , ldap . SCOPE_SUBTREE , \"(uid= %(user)s )\" ) LDAP_CA_FILE_PATH = \"/path/to/my/company-ca.crt\" AUTH_LDAP_CONNECTION_OPTIONS : { ldap . OPT_X_TLS_CACERTFILE : LDAP_CA_FILE_PATH , ldap . OPT_X_TLS_REQUIRE_CERT : ldap . OPT_X_TLS_ALLOW , ldap . OPT_X_TLS_NEWCTX : 0 } AUTH_LDAP_START_TLS : True AUTH_LDAP_USER_ATTR_MAP = { \"first_name\" : \"givenName\" , \"last_name\" : \"sn\" , \"email\" : \"uid\" }","title":"LDAP backend"},{"location":"squest_settings/#email","text":"Email settings are based on the django settings EMAIL_HOST = os . environ . get ( 'EMAIL_HOST' , None ) EMAIL_PORT = 25","title":"Email"},{"location":"tower_settings/","text":"Tower configuration Squest will need a token in order to communicate with your Tower instance. Create an application on your Tower/AWX instance On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential Create a token for Squest application On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Tower"},{"location":"tower_settings/#tower-configuration","text":"Squest will need a token in order to communicate with your Tower instance.","title":"Tower configuration"},{"location":"tower_settings/#create-an-application-on-your-towerawx-instance","text":"On Tower, go in Application menu and create a new app with the following configuration: name: squest Organization: Default Authorization grant type: Resource owner password based Client type: Confidential","title":"Create an application on your Tower/AWX instance"},{"location":"tower_settings/#create-a-token-for-squest-application","text":"On Tower/AWX: Go in your User Details page (top right corner), go into the tokens section Click add button Search for the \"squest\" application created previously and select it Give a scope \"write\" Save Copy the generated token. This will be the token to give to Squest when creating a new Tower server instance.","title":"Create a token for Squest application"},{"location":"contribute/code/","text":"Contributing: code The community can contribute to Squest by providing some new features, bug fix and enhancements. How to contribute Fork it! Checkout the dev branch git checkout dev Create your feature branch: git checkout -b my-new-feature Commit your changes: git commit -am 'Add some feature' Push to the branch: git push origin my-new-feature Submit a pull request in the dev branch If you are new on Github environment, we recommend you to read the first contribution guide . Follow the development environment setup documentation to prepare your workstation with prerequisites. Constraints Respect PEP 257 -- Docstring conventions. For each class or method add a description with summary, input parameter, returned parameter, type of parameter def my_method ( my_parameter ): \"\"\" Description of he method :param my_parameter: description of he parameter :type my_parameter: str \"\"\" pass Respect PEP 8 -- Style Guide for Python Code We recommend the usage of an IDE like Pycharm","title":"Code"},{"location":"contribute/code/#contributing-code","text":"The community can contribute to Squest by providing some new features, bug fix and enhancements. How to contribute Fork it! Checkout the dev branch git checkout dev Create your feature branch: git checkout -b my-new-feature Commit your changes: git commit -am 'Add some feature' Push to the branch: git push origin my-new-feature Submit a pull request in the dev branch If you are new on Github environment, we recommend you to read the first contribution guide . Follow the development environment setup documentation to prepare your workstation with prerequisites.","title":"Contributing: code"},{"location":"contribute/code/#constraints","text":"Respect PEP 257 -- Docstring conventions. For each class or method add a description with summary, input parameter, returned parameter, type of parameter def my_method ( my_parameter ): \"\"\" Description of he method :param my_parameter: description of he parameter :type my_parameter: str \"\"\" pass Respect PEP 8 -- Style Guide for Python Code We recommend the usage of an IDE like Pycharm","title":"Constraints"},{"location":"contribute/documentation/","text":"Contributing to the documentation The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve -a 0 .0.0.0:4000 The page is available on http://127.0.0.1:4000 . Send a pull request then to propose your changes to the project. Notes Reset your gh-pages branch to match the upstream If you've built mkdocs and published a version in your fork for testing, your gh-pages branch will differ from the upstream repository. To reset your local gh-pages , follow the procedure below: # delete local branch git branch -D gh-pages # delete remote branch (fork here is your remote. Replace by origin if needed) git push -d fork gh-pages # checkout gh-pages git checkout --orphan gh-pages # pull last version (upstream is the remote name of the main repo) git pull upstream gh-pages # (optional) force push to your fork to override changes git push -f fork gh-pages # go back to your original branch git checkout master","title":"Documentation"},{"location":"contribute/documentation/#contributing-to-the-documentation","text":"The documentation is written in markdown and then generated with mkdocs . Required libraries are installed if you've followed the development environment documentation of the project. Graphs and diagrams are generated by the Mermaid library . Update the documentation in the docs folder placed in the root of the project. Run dev server locally to check the result mkdocs serve -a 0 .0.0.0:4000 The page is available on http://127.0.0.1:4000 . Send a pull request then to propose your changes to the project.","title":"Contributing to the documentation"},{"location":"contribute/documentation/#notes","text":"","title":"Notes"},{"location":"contribute/documentation/#reset-your-gh-pages-branch-to-match-the-upstream","text":"If you've built mkdocs and published a version in your fork for testing, your gh-pages branch will differ from the upstream repository. To reset your local gh-pages , follow the procedure below: # delete local branch git branch -D gh-pages # delete remote branch (fork here is your remote. Replace by origin if needed) git push -d fork gh-pages # checkout gh-pages git checkout --orphan gh-pages # pull last version (upstream is the remote name of the main repo) git pull upstream gh-pages # (optional) force push to your fork to override changes git push -f fork gh-pages # go back to your original branch git checkout master","title":"Reset your gh-pages branch to match the upstream"},{"location":"dev/db-erd/","text":"Database Entity Relationship Diagrams erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type json enabled_survey_fields bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date date_message string content } SUPPORT_MESSAGE { date date_message string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database ERD"},{"location":"dev/db-erd/#database-entity-relationship-diagrams","text":"erDiagram TOWER_SERVER { string name string host string token bool secure bool ssl_verify } JOB_TEMPLATE { string name int tower_id json survey } OPERATION { string name string description enum type json enabled_survey_fields bool auto_accept bool auto_process int process_timeout_second } SERVICE { string name string description blob image } REQUEST { json fill_in_survey date date_submitted date date_complete int tower_job_id enum state datetime periodic_task_date_expire string failure_message } INSTANCE { string name json spec enum state } SUPPORT { string title enum state date date_opened date date_closed } REQUEST_MESSAGE { date date_message string content } SUPPORT_MESSAGE { date date_message string content } JOB_TEMPLATE ||--o{ TOWER_SERVER: has OPERATION ||--o{ JOB_TEMPLATE: has OPERATION ||--o{ SERVICE: has REQUEST ||--o{ OPERATION: has REQUEST |o--|| PERDIODIC_TASK: has REQUEST ||--o{ INSTANCE: has INSTANCE }|--o{ USER: has SUPPORT ||--o{ INSTANCE: has SUPPORT ||--o{ USER: openned_by REQUEST_MESSAGE ||--o{ REQUEST: has REQUEST_MESSAGE ||--o{ USER: from SUPPORT_MESSAGE ||--o{ SUPPORT: has SUPPORT_MESSAGE ||--o{ USER: from","title":"Database Entity Relationship Diagrams"},{"location":"dev/dev-env/","text":"Setup a development environment Pre requisites Tools Following tools need to be installed on your workstation: Docker Docker-compose Python 3.8 Python virtualenv Poetry npm System packages Ubuntu based OS: sudo apt-get install libmysqlclient-dev graphviz CentOS/RedHat/Fedora sudo yum install mysql-devel graphviz Start a development environment The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API Docker compose Run the Docker dev env to bring up database, message broker and other required system docker-compose -f dev-env.docker-compose.yml up Javascript libraries Install JS libs (npm need to be installed) npm install Python environment Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data Celery worker and periodic task scheduler Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat --scheduler django -l info Django integrated web server This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver Commands To clean all Celery pending tasks poetry shell celery -A restapi purge Execute tests Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test coverage report phpMyAdmin phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"Setup a dev env"},{"location":"dev/dev-env/#setup-a-development-environment","text":"","title":"Setup a development environment"},{"location":"dev/dev-env/#pre-requisites","text":"","title":"Pre requisites"},{"location":"dev/dev-env/#tools","text":"Following tools need to be installed on your workstation: Docker Docker-compose Python 3.8 Python virtualenv Poetry npm","title":"Tools"},{"location":"dev/dev-env/#system-packages","text":"Ubuntu based OS: sudo apt-get install libmysqlclient-dev graphviz CentOS/RedHat/Fedora sudo yum install mysql-devel graphviz","title":"System packages"},{"location":"dev/dev-env/#start-a-development-environment","text":"The development environment is composed of 4 parts: Docker compose: The Docker compose file is used to deploy all required components such as the database and the message broker Celery worker: The Celery worker is a separated process that receive tasks from the main Django process to be executed asynchronously Celery beat: Celery beat is a periodic task scheduler that send task into the celery worker based on a frequency. This part is used by Squest to check the status of executed Tower job Django built in web server: Integrated web server used only for development purpose. main process of the application that serve the Web Ui and the API","title":"Start a development environment"},{"location":"dev/dev-env/#docker-compose","text":"Run the Docker dev env to bring up database, message broker and other required system docker-compose -f dev-env.docker-compose.yml up","title":"Docker compose"},{"location":"dev/dev-env/#javascript-libraries","text":"Install JS libs (npm need to be installed) npm install","title":"Javascript libraries"},{"location":"dev/dev-env/#python-environment","text":"Initializing and installing python libraries with Poetry poetry install Go into the python virtual env poetry shell Create the database with Django migration script python manage.py migrate Collect static files python manage.py collectstatic --noinput Insert default data python manage.py insert_default_data","title":"Python environment"},{"location":"dev/dev-env/#celery-worker-and-periodic-task-scheduler","text":"Run Celery process for async tasks from a new terminal poetry shell celery -A service_catalog worker -l info Run Celery beat for periodic tasks from a new terminal poetry shell celery -A service_catalog worker --beat --scheduler django -l info","title":"Celery worker and periodic task scheduler"},{"location":"dev/dev-env/#django-integrated-web-server","text":"This next command should be executed from your IDE. Run django dev server poetry shell python manage.py runserver","title":"Django integrated web server"},{"location":"dev/dev-env/#commands","text":"To clean all Celery pending tasks poetry shell celery -A restapi purge","title":"Commands"},{"location":"dev/dev-env/#execute-tests","text":"Run unit tests poetry shell python manage.py test Run code coverage coverage run --source = '.' manage.py test coverage report","title":"Execute tests"},{"location":"dev/dev-env/#phpmyadmin","text":"phpMyAdmin is exposed on localhost:8082. server : db user : root password : p@ssw0rd","title":"phpMyAdmin"},{"location":"dev/instance-state-machine/","text":"Instance state machine graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/instance-state-machine/#instance-state-machine","text":"graph TB start((Start)) start --> pending pending[PENDING] provisioning[PROVISIONING] provision_failed[PROVISION_FAILED] available[AVAILABLE] updating[UPDATING] update_failed[UPDATE_FAILED] deleting[DELETING] delete_failed[DELETE_FAILED] deleted[DELETED] archived[ARCHIVED] pending --> provisioning provision_ok{provision ok?} style provision_ok fill:#80CBC4 provisioning --> provision_ok provision_ok --> |No| provision_failed provision_ok --> |Yes| available provision_failed --> |retry| provisioning available --> |update| updating update_ok{update ok?} style update_ok fill:#80CBC4 updating --> update_ok update_ok --> |No| update_failed update_ok --> |Yes| available available --> |Delete| deleting deletion_ok{deletion ok?} style deletion_ok fill:#80CBC4 deleting --> deletion_ok deletion_ok --> |No| delete_failed deletion_ok --> |Yes| deleted deleted --> |archive| archived delete_failed --> |Retry| deleting","title":"Instance state machine"},{"location":"dev/request-state-machine/","text":"Request state machine graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted","title":"Request state machine"},{"location":"dev/request-state-machine/#request-state-machine","text":"graph TB start((Start)) submitted[SUBMITTED] start --> submitted auto_accept{auto accept?} style auto_accept fill:#80CBC4 instance_pending([instance pending]) submitted --> instance_pending instance_pending --> auto_accept accepted[ACCEPTED] auto_accept -->|Yes| accepted admin_action_1{admin action} style admin_action_1 fill:#80DEEA auto_accept -->|No| admin_action_1 need_info[NEED_INFO] admin_action_1 -->|need_info| need_info admin_action_1 -->|accept| accepted rejected[REJECTED] need_info -->|reject| rejected need_info -->|Submit| submitted canceled[CANCELED] need_info --> |cancel|canceled rejected --> |cancel|canceled submitted --> |cancel|canceled canceled --> |delete| deleted deleted((Deleted)) auto_pocess{auto process?} style auto_pocess fill:#80CBC4 accepted --> auto_pocess auto_pocess --> |Yes| operation_type admin_action_2{admin action} auto_pocess --> |No| admin_action_2 admin_action_2 --> |process| operation_type style admin_action_2 fill:#80DEEA operation_type{Operation type?} style operation_type fill:#80CBC4 instance_creating([instance_creating]) instance_updating([instance_updating]) instance_deleting([instance_deleting]) operation_type --> |CREATE| instance_creating operation_type --> |UPDATE| instance_updating operation_type --> |DELETE| instance_deleting processing[PROCESSING] instance_creating --> processing instance_updating --> processing instance_deleting --> processing processing_ok{processing ok?} style processing_ok fill:#80CBC4 processing --> processing_ok complete[COMPLETE] failed[FAILED] processing_ok --> |Yes| complete processing_ok --> |No| failed failed --> |retry| processing failed --> |cancel| accepted","title":"Request state machine"},{"location":"manual/billing_groups/","text":"Billing groups Note: Billing groups is an optional feature. Billing groups allow Squest administrator to visualize who is consuming what. Billing groups management From the sidebar, click on 'Billing Groups' to go on the billing groups page. Administrators can: See billing groups list with name and the user count of each billing group. Create a billing group. Manage users in billing groups. Delete billing groups. Define billing group in a service By default Squest services are not linked to any billing group. Administrators can describe how the billing of the service will be predefined. Through the service form, choices are: Administrator defined billing group User defined billing group from his billing group User defined billing group from all billing group Administrator defined billing group Administrators select a fixed billing group (can be none). Each created instance will be linked to this billing group. Administrators can also hide the billing from the end users. The billing group will not be asked neither shown in the end user form when requesting the service. User defined billing group From his billing group Administrators let the end user choose from his billing group when he sends an instance request. Note: users without billing groups can not request this service. From all billing group Administrators let the end user choose from all billing group when he requests this service.","title":"Billing groups"},{"location":"manual/billing_groups/#billing-groups","text":"Note: Billing groups is an optional feature. Billing groups allow Squest administrator to visualize who is consuming what.","title":"Billing groups"},{"location":"manual/billing_groups/#billing-groups-management","text":"From the sidebar, click on 'Billing Groups' to go on the billing groups page. Administrators can: See billing groups list with name and the user count of each billing group. Create a billing group. Manage users in billing groups. Delete billing groups.","title":"Billing groups management"},{"location":"manual/billing_groups/#define-billing-group-in-a-service","text":"By default Squest services are not linked to any billing group. Administrators can describe how the billing of the service will be predefined. Through the service form, choices are: Administrator defined billing group User defined billing group from his billing group User defined billing group from all billing group","title":"Define billing group in a service"},{"location":"manual/billing_groups/#administrator-defined-billing-group","text":"Administrators select a fixed billing group (can be none). Each created instance will be linked to this billing group. Administrators can also hide the billing from the end users. The billing group will not be asked neither shown in the end user form when requesting the service.","title":"Administrator defined billing group"},{"location":"manual/billing_groups/#user-defined-billing-group","text":"","title":"User defined billing group"},{"location":"manual/billing_groups/#from-his-billing-group","text":"Administrators let the end user choose from his billing group when he sends an instance request. Note: users without billing groups can not request this service.","title":"From his billing group"},{"location":"manual/billing_groups/#from-all-billing-group","text":"Administrators let the end user choose from all billing group when he requests this service.","title":"From all billing group"},{"location":"manual/resource_tracking/","text":"Resource tracking Nowadays, IT infrastructures are composed of multiple layers. Physical servers, virtual machines, containers, storage,... Each layer is consumer or a producer of resources of another layer. As an IT administrator, we need to monitor resource consumption of a top layers to be sure that we can provide services on underlying layers. The resource tracking feature allows to monitor reserved resources and highlight available resource in an infrastructure. Concept To introduce the concept of resources, lets take the example of a virtualization stack like a VMware vCenter. The stack is composed of physical servers that produce resources like CPU and memory to the hypervisor. Then we can create VMs that will consume those resources as virtual CPU( vCPU ) and memory . The hypervisor is then a resource pool with producers and consumers of resources. If we want to add more VMs that consume resources from the hypervisor resource pool, we need to be sure we have enough physical servers that produce into it. This stack could be drawn like the following with resource group (blue), resource pool (orange) and resources (green): In this example, we have: two resources of the same kind(server) that produce resources into the resource pool hypervisor two resources of the same kind(VM) that consume resources from the resource pool hypervisor As server resources are of the same kind and produce in the same pool on the same attributes , they belong to the same resource group servers . As VM resources are of the same kind and consume in the same pool on the same attributes , they belong to the same resource group VMs . The previous drawing could be then simplified by just showing resource groups and resource pool : Generic objects Resource pool A resource pool is a generic object composed by attributes . Resource pool attributes have producers and consumers which are attributes from resource groups . The sum of all resource group attributes that produce in the same resource pool attribute give a total produced . The sum of all resource group attributes that consume in the same resource pool attribute give a total consumed . The difference between total produced and total consumed gives the amount of available resources for a particular resource pool attribute . Resource group A resource group is a definition of a resource of same kind, composed of attributes that may produce or consume from a particular resource pool . Example of resource group: Bare metal servers Physical disks VMware VMs K8S namespaces Openshift projects Openstack tenants Resource A resource is an instance of a resource group definition. Resources can be created, updated or deleted from the Squest UI or API. Updating resources in a resource group impact the total amount of produced or consumed resource on pool's attributes. Multiple layer example In this example we do track the consumption of an orchestrator of container like Kubernetes or Openshift. Namespaces (or projects in Openshift world) are a way to divide cluster resources between multiple users by using resource quota. Openshift and Kubernetes frameworks are commonly deployed in a virtual machines. So we retrieve layers from previous example with bare metal servers that produce resources in an hypervisor. Orchestrators are usually composed of 2 kind of node: Masters and Workers. Master VMs are used by the infrastructure itself and workers for user's workloads, aka namespaces. As namespaces are only executed in \"worker\" nodes, we need to declare 2 different resource group : Master VMs Worker VMs The aggregation of resources of all workers compose the resource pool of available resources that the namespaces resource group will consume. The complete resource tracking definition would look like the following: With this definition, we are able to determine there is enough available resources in pools to handle underlying objects. Adding a new namespace in the last resource group namespaces will generate more consumption on the K8S resource pool . If this last pool is lacking of resources, adding more worker node in the worker VMs resource group will be required, generating consumption on upper layers and so on... Link service catalog instances to resources Resources can be created from the API. It allows to create automatically a new resource in a resource group when something is provisioned from the service catalog. In the example below, the playbook executed in Tower/AWX would have created a VM. At the end of the process we call the squest API to instantiate a resource in the right resource group to reflect the consumption. We link as well the pending instance(given by squest.instance.id ) to this resource via the flag service_catalog_instance . - name : Add resource in resource group example hosts : localhost connection : local gather_facts : false vars : squest_api : \"http://127.0.0.1:8000/api/\" resource_group_vm_id : 8 squest : # this would be the sent data from squest as extra vars instance : id : 8 name : test service : 1 spec : { } state : PROVISIONING vm_name : \"test-vm\" vm_vcpu : 4 vm_memory : 16 tasks : - name : Print info sent by Squest debug : var : squest # ----------------------- # PLACE HERE ALL THE MAGIC TO CREATE THE RESOURCE # ----------------------- - name : Create a resource in squest uri : url : \"{{ squest_api }}resource_tracker/resource_group/{{ resource_group_vm_id }}/resources/\" user : \"admin\" password : \"admin\" method : POST body : name : \"{{ vm_name }}\" service_catalog_instance : \"{{ squest['instance']['id'] }}\" attributes : - name : \"vCPU\" value : \"{{ vm_vcpu }}\" - name : \"Memory\" value : \"{{ vm_memory }}\" force_basic_auth : yes status_code : 201 body_format : json","title":"Resource tracking"},{"location":"manual/resource_tracking/#resource-tracking","text":"Nowadays, IT infrastructures are composed of multiple layers. Physical servers, virtual machines, containers, storage,... Each layer is consumer or a producer of resources of another layer. As an IT administrator, we need to monitor resource consumption of a top layers to be sure that we can provide services on underlying layers. The resource tracking feature allows to monitor reserved resources and highlight available resource in an infrastructure.","title":"Resource tracking"},{"location":"manual/resource_tracking/#concept","text":"To introduce the concept of resources, lets take the example of a virtualization stack like a VMware vCenter. The stack is composed of physical servers that produce resources like CPU and memory to the hypervisor. Then we can create VMs that will consume those resources as virtual CPU( vCPU ) and memory . The hypervisor is then a resource pool with producers and consumers of resources. If we want to add more VMs that consume resources from the hypervisor resource pool, we need to be sure we have enough physical servers that produce into it. This stack could be drawn like the following with resource group (blue), resource pool (orange) and resources (green): In this example, we have: two resources of the same kind(server) that produce resources into the resource pool hypervisor two resources of the same kind(VM) that consume resources from the resource pool hypervisor As server resources are of the same kind and produce in the same pool on the same attributes , they belong to the same resource group servers . As VM resources are of the same kind and consume in the same pool on the same attributes , they belong to the same resource group VMs . The previous drawing could be then simplified by just showing resource groups and resource pool :","title":"Concept"},{"location":"manual/resource_tracking/#generic-objects","text":"","title":"Generic objects"},{"location":"manual/resource_tracking/#resource-pool","text":"A resource pool is a generic object composed by attributes . Resource pool attributes have producers and consumers which are attributes from resource groups . The sum of all resource group attributes that produce in the same resource pool attribute give a total produced . The sum of all resource group attributes that consume in the same resource pool attribute give a total consumed . The difference between total produced and total consumed gives the amount of available resources for a particular resource pool attribute .","title":"Resource pool"},{"location":"manual/resource_tracking/#resource-group","text":"A resource group is a definition of a resource of same kind, composed of attributes that may produce or consume from a particular resource pool . Example of resource group: Bare metal servers Physical disks VMware VMs K8S namespaces Openshift projects Openstack tenants","title":"Resource group"},{"location":"manual/resource_tracking/#resource","text":"A resource is an instance of a resource group definition. Resources can be created, updated or deleted from the Squest UI or API. Updating resources in a resource group impact the total amount of produced or consumed resource on pool's attributes.","title":"Resource"},{"location":"manual/resource_tracking/#multiple-layer-example","text":"In this example we do track the consumption of an orchestrator of container like Kubernetes or Openshift. Namespaces (or projects in Openshift world) are a way to divide cluster resources between multiple users by using resource quota. Openshift and Kubernetes frameworks are commonly deployed in a virtual machines. So we retrieve layers from previous example with bare metal servers that produce resources in an hypervisor. Orchestrators are usually composed of 2 kind of node: Masters and Workers. Master VMs are used by the infrastructure itself and workers for user's workloads, aka namespaces. As namespaces are only executed in \"worker\" nodes, we need to declare 2 different resource group : Master VMs Worker VMs The aggregation of resources of all workers compose the resource pool of available resources that the namespaces resource group will consume. The complete resource tracking definition would look like the following: With this definition, we are able to determine there is enough available resources in pools to handle underlying objects. Adding a new namespace in the last resource group namespaces will generate more consumption on the K8S resource pool . If this last pool is lacking of resources, adding more worker node in the worker VMs resource group will be required, generating consumption on upper layers and so on...","title":"Multiple layer example"},{"location":"manual/resource_tracking/#link-service-catalog-instances-to-resources","text":"Resources can be created from the API. It allows to create automatically a new resource in a resource group when something is provisioned from the service catalog. In the example below, the playbook executed in Tower/AWX would have created a VM. At the end of the process we call the squest API to instantiate a resource in the right resource group to reflect the consumption. We link as well the pending instance(given by squest.instance.id ) to this resource via the flag service_catalog_instance . - name : Add resource in resource group example hosts : localhost connection : local gather_facts : false vars : squest_api : \"http://127.0.0.1:8000/api/\" resource_group_vm_id : 8 squest : # this would be the sent data from squest as extra vars instance : id : 8 name : test service : 1 spec : { } state : PROVISIONING vm_name : \"test-vm\" vm_vcpu : 4 vm_memory : 16 tasks : - name : Print info sent by Squest debug : var : squest # ----------------------- # PLACE HERE ALL THE MAGIC TO CREATE THE RESOURCE # ----------------------- - name : Create a resource in squest uri : url : \"{{ squest_api }}resource_tracker/resource_group/{{ resource_group_vm_id }}/resources/\" user : \"admin\" password : \"admin\" method : POST body : name : \"{{ vm_name }}\" service_catalog_instance : \"{{ squest['instance']['id'] }}\" attributes : - name : \"vCPU\" value : \"{{ vm_vcpu }}\" - name : \"Memory\" value : \"{{ vm_memory }}\" force_basic_auth : yes status_code : 201 body_format : json","title":"Link service catalog instances to resources"},{"location":"manual/service_catalog/","text":"Lifecycle management Provisioning a service When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : request : instance : id : 1 name : test service : 1 spec : {} state : PROVISIONING file_name : foo.conf Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project) Playbook example In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job tempalate survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}service_catalog/admin/instance/{{ squest['request']['instance']['id'] }}/\" # do not forget the last slash user : \"admin\" password : \"admin\" method : PATCH body : spec : file_name : \"{{ file_name }}\" force_basic_auth : yes status_code : 200 body_format : json Day 2 operations Day 2 operations are operations that update or delete existing resources. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated Playbook example Example of extra vars sent by squest: squest : request : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job tempalate survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['request']['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes","title":"Service catalog"},{"location":"manual/service_catalog/#lifecycle-management","text":"","title":"Lifecycle management"},{"location":"manual/service_catalog/#provisioning-a-service","text":"When a user request for the first time a service, an instance is created automatically and set to \"pending\" state on Squest. Once approved by the administrator, the request is sent to Tower to execute the linked job template. The executed job, aka the Ansible playbook, need to call back the Squest API in order to attach information (spec) to the pending instance. Squest provisioning workflow: sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request service Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec <br> {'uuid': 34, 'name': 'instance_name'} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service ready The playbook will receive a squest extra variable that contains information of to the pending instance linked to the request in addition to all extra variables which come from the survey of the job template. Example of extra variables sent by Squest: squest : request : instance : id : 1 name : test service : 1 spec : {} state : PROVISIONING file_name : foo.conf Specs related to the created instance are important in order to be sent later to a playbook in charge of updating this particular instance. Sent specs must contain unique IDs that allow to identify precisely the instance. (E.g: uuid of a VMware VM, namespace and cluster_api_url for an Openshift project)","title":"Provisioning a service"},{"location":"manual/service_catalog/#playbook-example","text":"In the example below, we've configured a job template with a survey that ask for a variable named file_name . The playbook will: create the resource (the file) call Squest api to link spec of the created resource to the instance - name : Create a file hosts : localhost connection : local gather_facts : false vars : squest_api_url : \"http://192.168.58.128:8000/api/\" tasks : - name : Print the job tempalate survey variable debug : var : file_name - name : Print info sent by Squest debug : var : squest - name : Create a file with the given file_name ansible.builtin.file : path : \"/tmp/{{ file_name }}\" owner : user group : user mode : '0644' state : touch - name : Update spec of the instance via the squest API uri : url : \"{{ squest_api_url }}service_catalog/admin/instance/{{ squest['request']['instance']['id'] }}/\" # do not forget the last slash user : \"admin\" password : \"admin\" method : PATCH body : spec : file_name : \"{{ file_name }}\" force_basic_auth : yes status_code : 200 body_format : json","title":"Playbook example"},{"location":"manual/service_catalog/#day-2-operations","text":"Day 2 operations are operations that update or delete existing resources. When a user creates a request for a day 2 operation of a provisioned instance, Squest automatically attach an extra_vars named squest that contains the instance spec sent by the playbook used to provision at first the resource. The playbook used to update the instance need to use info placed in squest variable to retrieve the real resource that need to be updated or deleted. The update playbook can send a new version of the instance to squest at the end of its process if required. sequenceDiagram participant User participant Admin participant Squest participant Tower User->>Squest: Request update Admin->>Squest: Approve Admin->>Squest: Process Squest->>Tower: Process - Extra vars:<br> {'squest': {'uuid': 34, 'name': 'instance_name'}} Squest-->>Tower: Check Note right of Tower: Running Tower->>Squest: Instance spec update <br> {'uuid': 34, 'name': 'instance_new_name} Squest-->>Tower: Check Note right of Tower: Successful Squest->>User: Notify service updated","title":"Day 2 operations"},{"location":"manual/service_catalog/#playbook-example_1","text":"Example of extra vars sent by squest: squest : request : instance : id : 1 name : test-instance service : 1 spec : file_name : foo.conf state : UPDATING string_to_place_in_file : \"this is a string\" In the example below, the update job template survey ask for a string_to_place_in_file variable. The playbook receive as well all information that help to retrieve the resource to update. In this example the resource is the file_name . - name : Update content of a file hosts : localhost connection : local gather_facts : false tasks : - name : Print the job tempalate survey variable debug : var : string_to_place_in_file - name : Print info sent by Squest debug : var : squest - name : Add content into the file_name given by squest instance spec ansible.builtin.lineinfile : path : \"/tmp/{{ squest['request']['instance']['spec']['file_name'] }}\" line : \"{{ string_to_place_in_file }}\" create : yes","title":"Playbook example"},{"location":"manual/settings/","text":"Settings Global Hooks Global hooks are a way to call a Tower/AWX job template following the new state of a Request or an Instance . For example, if you want to call a job template that performs an action everytime a Request switch to FAILED state. Form field: name: Name of your hook Model: Target model object that will be linked to the hook ( Request or an Instance ) State: State of the selected model . The hook will be triggered when an instance of the select model type will switch to this selected state Job template: The Tower/AWX job template to execute when an instance of the selected model reach the selected state Extra vars: extra variable as JSON to add to the selected job template States documentation: Available states for a Request . Available states for a Instance .","title":"Settings"},{"location":"manual/settings/#settings","text":"","title":"Settings"},{"location":"manual/settings/#global-hooks","text":"Global hooks are a way to call a Tower/AWX job template following the new state of a Request or an Instance . For example, if you want to call a job template that performs an action everytime a Request switch to FAILED state. Form field: name: Name of your hook Model: Target model object that will be linked to the hook ( Request or an Instance ) State: State of the selected model . The hook will be triggered when an instance of the select model type will switch to this selected state Job template: The Tower/AWX job template to execute when an instance of the selected model reach the selected state Extra vars: extra variable as JSON to add to the selected job template States documentation: Available states for a Request . Available states for a Instance .","title":"Global Hooks"}]}